<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>AI笔记（5）：神经网络应用 | Olstussy</title><meta name="author" content="Olstussy"><meta name="copyright" content="Olstussy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1.数据划分：训练 &#x2F; 验证 &#x2F; 测试集1.1 深度学习实践迭代优化实际应用深度学习是一个迭代过程。 在构建一个神经网络的时候，我们需要设置许多超参数，例如神经网络的层数 (#Layers) 、每个隐藏层包含的神经元个数 (#Hidden Units) 、学习速率 (Learning Rates) 、激活函数 (Activation Functions) 的选择等。实际上很">
<meta property="og:type" content="article">
<meta property="og:title" content="AI笔记（5）：神经网络应用">
<meta property="og:url" content="https://olstussy.top/2023/08/03/5.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8/index.html">
<meta property="og:site_name" content="Olstussy">
<meta property="og:description" content="1.数据划分：训练 &#x2F; 验证 &#x2F; 测试集1.1 深度学习实践迭代优化实际应用深度学习是一个迭代过程。 在构建一个神经网络的时候，我们需要设置许多超参数，例如神经网络的层数 (#Layers) 、每个隐藏层包含的神经元个数 (#Hidden Units) 、学习速率 (Learning Rates) 、激活函数 (Activation Functions) 的选择等。实际上很">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/08/02/UOWRIZPuaAX81yL.jpg">
<meta property="article:published_time" content="2023-08-03T10:41:07.000Z">
<meta property="article:modified_time" content="2023-08-04T15:25:26.375Z">
<meta property="article:author" content="Olstussy">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/08/02/UOWRIZPuaAX81yL.jpg"><link rel="shortcut icon" href="/img/favicon-32x32-next.png"><link rel="canonical" href="https://olstussy.top/2023/08/03/5.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'AI笔记（5）：神经网络应用',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-08-04 23:25:26'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2023/08/02/UOWRIZPuaAX81yL.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2023/08/02/n2kWORi8zyCmNXH.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Olstussy"><span class="site-name">Olstussy</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">AI笔记（5）：神经网络应用</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-03T10:41:07.000Z" title="发表于 2023-08-03 18:41:07">2023-08-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-08-04T15:25:26.375Z" title="更新于 2023-08-04 23:25:26">2023-08-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="AI笔记（5）：神经网络应用"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2023/08/03/5.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2023/08/03/5.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8/" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><meta name="referrer" content="no-referrer">

<h2 id="1-数据划分：训练-验证-测试集"><a href="#1-数据划分：训练-验证-测试集" class="headerlink" title="1.数据划分：训练 &#x2F; 验证 &#x2F; 测试集"></a>1.数据划分：训练 &#x2F; 验证 &#x2F; 测试集</h2><h3 id="1-1-深度学习实践迭代优化"><a href="#1-1-深度学习实践迭代优化" class="headerlink" title="1.1 深度学习实践迭代优化"></a>1.1 深度学习实践迭代优化</h3><p>实际应用深度学习是一个迭代过程。</p>
<p>在构建一个神经网络的时候，我们需要设置许多超参数，例如<strong>神经网络的层数</strong> (#Layers) 、每个<strong>隐藏层包含的神经元个数</strong> (#Hidden Units) 、<strong>学习速率</strong> (Learning Rates) 、<strong>激活函数</strong> (Activation Functions) 的选择等。实际上很难在第一次设置的时候就选择到这些最佳的超参数，而是需要通过不断地迭代更新来获得。</p>
<p>循环迭代过程是如下这样的：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/6435362d866ce37b006387d0ef48634d.png" alt="深度学习优化迭代过程"></p>
<ol start="11">
<li>产生想法<strong>Idea</strong>，选择初始的参数值，构建神经网络模型结构；</li>
<li>通过代码<strong>Code</strong>实现上述想法；</li>
<li>通过实验<strong>Experiment</strong>验证这些超参数对应的神经网络的表现性能。</li>
<li>根据验证结果，我们对超参数进行适当的调整优化，再进行下一次的<strong>Idea-&gt;Code-&gt;Experiment</strong>循环。通过很多次的循环，不断调整超参数，选定最佳的参数值，从而让神经网络性能最优化。</li>
</ol>
<p>上述迭代过程中，决定整个训练过程快慢的关键在于<strong>单次循环所花费的时间</strong>，单次循环越快，训练过程越快。而设置合适的<strong>训练集</strong> (Training sets) 、<strong>验证集</strong> (Development sets) 、<strong>测试集</strong> (Test sets) 大小，能有效提高训练效率。</p>
<p>上述数据部分来源于建立模型的过程中，我们对于总体数据的划分：</p>
<ul>
<li><strong>训练集 (Training Sets)</strong> ：用训练集对算法或模型进行训练过程。</li>
<li><strong>验证集 ( (Development Sets)</strong> ：利用验证集 (又称为简单交叉验证集，hold-out cross validation set) 进行<strong>交叉验证，选择出最好的模型</strong>。</li>
<li><strong>测试集 (Test Sets)</strong> ：最后利用测试集对模型进行测试，获取模型运行的无偏估计 (对学习方法进行评估) 。</li>
</ul>
<h3 id="1-2-前大数据时代划分方式"><a href="#1-2-前大数据时代划分方式" class="headerlink" title="1.2 前大数据时代划分方式"></a>1.2 前大数据时代划分方式</h3><p>在小数据量的时代，如100、1000、10000的数据量大小，可以将数据集按照以下比例进行划分：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/dc724ddd049bd10b37276dd8abf8db14.png" alt="深度学习中的数据切分"></p>
<ul>
<li><strong>无验证集</strong>的情况：70%、30%</li>
<li><strong>有验证集</strong>的情况：60%、20%、20%</li>
</ul>
<h3 id="1-3-大数据时代划分方式"><a href="#1-3-大数据时代划分方式" class="headerlink" title="1.3 大数据时代划分方式"></a>1.3 大数据时代划分方式</h3><p>而在如今的大数据时代，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。</p>
<h4 id="关于验证集"><a href="#关于验证集" class="headerlink" title="关于验证集"></a>关于验证集</h4><p>验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约2-10种算法哪种更好，而不需要使用20%的数据作为验证集。如百万数据中抽取1万的数据作为验证集就可以了。</p>
<h4 id="关于测试集"><a href="#关于测试集" class="headerlink" title="关于测试集"></a>关于测试集</h4><p>测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中10000条数据也足以评估单个模型的效果。</p>
<p>我们针对不同量级的大数据场景，可以采用如下的训练集 (Training sets) 、验证集 (Development sets) 、测试集 (Test sets) 数据<strong>划分方式</strong>：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/b1863ea32bf92e46c37bf2bac7238987.png" alt="深度学习中的数据切分"></p>
<ul>
<li><strong>100万数据量</strong>：98%、1%、1%</li>
<li><strong>超百万数据量</strong>：99.5%、0.25%、0.25% (或者99.5%、0.4%、0.1%)</li>
</ul>
<h3 id="1-4-数据划分建议"><a href="#1-4-数据划分建议" class="headerlink" title="1.4 数据划分建议"></a>1.4 数据划分建议</h3><p>建议验证集要和训练集来自于同一个分布 (数据来源一致) ，可以使得机器学习算法变得更快并获得更好的效果。</p>
<p><strong>假设你开发一个手机app，可以让用户上传图片，然后app识别出猫的图片</strong>。在app识别算法中，你的<strong>训练样本</strong>可能来自<strong>网络</strong>下载，而你的<strong>验证和测试样本</strong>可能来自不同用户的上传。从网络下载的图片一般像素较高而且比较正规，而用户上传的图片往往像素不稳定，且图片质量不一。这种情况下验证集和测试集的作用就受影响了。</p>
<p>如果不需要用<strong>无偏估计</strong>来评估模型的性能，则可以不需要测试集。</p>
<p><strong>Test sets测试集的目标主要是进行无偏估计</strong>。我们可以通过Train sets训练不同的算法模型，然后分别在Dev sets上进行验证，根据结果选择最好的算法模型。这样也是可以的，不需要再进行无偏估计了。如果只有Train sets和Dev sets，通常也有人把这里的Dev sets称为Test sets，我们要注意加以区别。</p>
<h2 id="2-模型估计：偏差-方差"><a href="#2-模型估计：偏差-方差" class="headerlink" title="2.模型估计：偏差 &#x2F; 方差"></a>2.模型估计：偏差 &#x2F; 方差</h2><h3 id="2-1-模型状态与评估"><a href="#2-1-模型状态与评估" class="headerlink" title="2.1 模型状态与评估"></a>2.1 模型状态与评估</h3><p><strong>偏差 (Bias)</strong> 和<strong>方差 (Variance)</strong> 是机器学习领域非常重要的两个概念和需要解决的问题。在传统的机器学习算法中，Bias和Variance是对立的，分别对应着欠拟合和过拟合，我们常常需要在Bias和Variance之间进行权衡。而在深度学习中，我们可以同时减小Bias和Variance，构建最佳神经网络模型。</p>
<p>我们先来梳理一下上面提到的概念：</p>
<ul>
<li><strong>偏差 (Bias)</strong> ：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力。</li>
<li><strong>方差 (Variance)</strong> ：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。</li>
<li><strong>噪声 (noise)</strong> ：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了<strong>学习问题本身的难度</strong>。</li>
</ul>
<p>如图是二维平面上二分类问题对应的几种模型状态 (High Bias，Just Right，High Variance) 示例图。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/7d1e174b52cbf7eb44f54cc2e6a49680.png" alt="模型估计：偏差/方差"></p>
<p>其中，<strong>High Bias对应着欠拟合</strong>，而<strong>High Variance对应着过拟合</strong>。在欠拟合 (underfitting) 的情况下，出现高偏差 (High Bias) 的情况，即不能很好地对数据进行分类。</p>
<p>这个例子中输入特征是二维的，High Bias和High Variance可以直接从图中分类线看出来。而<strong>对于输入特征是高维的情况，如何来判断是否出现了High Bias或者High Variance呢</strong>？</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/def768cf9fce584e155bd435de77dc80.png" alt="模型估计：偏差/方差"></p>
<p>这就要特别借助于上一节我们提到几个数据集的评估来完成了</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/40b427cd93898e7e9c1e2f4c6927cbf2.png" alt="模型估计：偏差/方差"></p>
<p>一般来说，训练集错误率体现了是否出现Bias (偏差) ，验证集 (和训练集差异) 错误率体现了是否出现Variance (方差) 。当训练出一个模型以后：</p>
<ul>
<li>训练集的错误率较小，而验证集的错误率却较大，说明模型存在较大方差 (Variance) ，可能出现了过拟合。</li>
<li>训练集和验证集的错误率都较大，且两者相当，说明模型存在较大偏差 (Bias) ，可能出现了欠拟合。</li>
<li>训练集错误率较大，且验证集的错误率远较训练集大，说明方差和偏差都较大，模型很差。</li>
<li>训练集和验证集的错误率都较小，且两者的相差也较小，说明方差和偏差都较小，这个模型效果比较好。</li>
</ul>
<p><strong>神经网络模型甚至可能出现High Bias and High Variance的糟糕状态</strong>，如下图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/a42fc5903db6a420e7a63ba4a8559cfc.png" alt="模型估计：偏差/方差"></p>
<h3 id="2-2-应对方法"><a href="#2-2-应对方法" class="headerlink" title="2.2 应对方法"></a>2.2 应对方法</h3><p>模型可能处于上述提到的不同状态中，在我们对模型状态评估完毕之后，针对不同的状态，优化方式如下：</p>
<ul>
<li>模型存在<strong>高偏差</strong>：扩大网络规模，如添加隐藏层或隐藏单元数目；寻找合适的网络架构，使用更大的NN结构；花费更长时间训练。</li>
<li>模型存在<strong>高方差</strong>：获取更多的数据；正则化 (Regularization) ；寻找更合适的网络结构。</li>
<li>不断尝试，直到找到低偏差、低方差的框架。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/img_convert/c39aa29a54a97f93303b872e5b58f4e4.png" alt="模型估计：偏差/方差"></p>
<p>在深度学习的早期阶段，没有太多方法能做到只减少偏差或方差而不影响到另外一方。而在大数据时代，深度学习对监督式学习大有裨益，使得我们不用像以前一样太过关注如何平衡偏差和方差的权衡问题，通过以上方法可以在不增加某一方的前提下减少另一方的值。</p>
<h2 id="3-正则化-Regularization"><a href="#3-正则化-Regularization" class="headerlink" title="3.正则化 (Regularization)"></a>3.正则化 (Regularization)</h2><h3 id="3-1-正则化"><a href="#3-1-正则化" class="headerlink" title="3.1 正则化"></a>3.1 正则化</h3><p>如果模型出现了过拟合 (High Variance) 状态，可以通过正则化Regularization来缓解解决。虽然扩大训练样本数量也是减小High Variance的一种方法，但是通常获得更多训练样本的成本太高，比较困难。所以，更可行有效的办法就是使用正则。</p>
<h3 id="3-2-Logistic-回归中的正则化"><a href="#3-2-Logistic-回归中的正则化" class="headerlink" title="3.2 Logistic 回归中的正则化"></a>3.2 Logistic 回归中的正则化</h3><p>我们先回顾一下之前介绍过的逻辑回归模型，我们在Cost Function里添加了<strong>L2 Regularization</strong> ，表达式如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=J(w,b)=%5Cfrac1m%20%5Csum_%7Bi=1%7D%5Em%20L(%5Chat%20y%5E%7B(i)%7D,y%5E%7B(i)%7D)+%5Cfrac%7B%5Clambda%7D%7B2m%7D%7C%7Cw%7C%7C_2%5E2" alt="公式"></p>
<blockquote>
<p><strong>为什么只对 w 进行正则化而不对b 进行正则化呢</strong>？</p>
<p>其实也可以对 <strong>b</strong>进行正则化。但是一般w的维度很大，而 <strong>b</strong>只是一个常数。相比较来说，参数很大程度上由 <strong>w</strong> 决定，改变 <strong>b</strong> 值对整体模型影响较小。所以，一般为了简便，就忽略对 <strong>b</strong> 的正则化了。</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/img_convert/3b4e96c266754c35c5939a43d17187c2.png" alt="正则化"></p>
<p>除了L2正则化之外，我们在逻辑回归中也可以<strong>添加L1正则化</strong>。表达式如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=J(w,b)=%5Cfrac1m%5Csum_%7Bi=1%7D%5EmL(%5Chat%20y%5E%7B(i)%7D,y%5E%7B(i)%7D)+%5Cfrac%7B%5Clambda%7D%7B2m%7D%7C%7Cw%7C%7C_1" alt="公式"></p>
<p>与L2正则化相比，<strong>L1正则化更容易得到稀疏的w 解</strong>，即最后训练得到的权重 w 中有很多为零值。L1正则化优点是节约存储空间 (因为大部分 w 为0) 。但实际上L1正则化在解决过拟合问题上并不优于L2正则化，且L1的在微分求导方面比较复杂。所以一般更常用的还是L2正则化。</p>
<p>L1、L2正则化中的 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="公式"> 就是正则化参数 (超参数的一种) 。可以设置 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="公式"> 为不同的值，在验证集Dev set中进行验证，选择最佳的 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="公式">。</p>
<h3 id="3-3-神经网络中的正则化"><a href="#3-3-神经网络中的正则化" class="headerlink" title="3.3 神经网络中的正则化"></a>3.3 神经网络中的正则化</h3><p>在深度学习模型中，<strong>L2正则化的表达式</strong>如图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/742b968fe0a1fe8725cc5328c8801dbf.png" alt="神经网络的正则化"></p>
<p>通常，我们把 <img src="https://www.zhihu.com/equation?tex=%7C%7Cw%5E%7B%5Bl%5D%7D%7C%7C%5E2" alt="公式"> 称为<strong>弗罗贝尼乌斯范数</strong> (Frobenius Norm) ，记为 <img src="https://www.zhihu.com/equation?tex=%7C%7Cw%5E%7B%5Bl%5D%7D%7C%7C_F%5E2" alt="公式">。</p>
<p>由于在Cost Function中加入了正则化项，梯度下降算法中的 <img src="https://www.zhihu.com/equation?tex=dw%5E%7B%5Bl%5D%7D" alt="公式"> 计算表达式需要做如下修改：</p>
<p><img src="https://www.zhihu.com/equation?tex=dw%5E%7B%5Bl%5D%7D=dw%5E%7B%5Bl%5D%7D_%7Bbefore%7D+%5Cfrac%7B%5Clambda%7D%7Bm%7Dw%5E%7B%5Bl%5D%7D" alt="公式"> <img src="https://www.zhihu.com/equation?tex=w%5E%7B%5Bl%5D%7D:=w%5E%7B%5Bl%5D%7D-%5Calpha%5Ccdot%20dw%5E%7B%5Bl%5D%7D" alt="公式"></p>
<p><strong>大家有时候也会听到L2正则化被称做weight decay</strong>。这是因为，由于加上了正则项，<img src="https://www.zhihu.com/equation?tex=dw%5E%7B%5Bl%5D%7D" alt="公式"> 有个增量，在更新 <img src="https://www.zhihu.com/equation?tex=w%5E%7B%5Bl%5D%7D" alt="公式"> 的时候，会多减去这个增量，使得 <img src="https://www.zhihu.com/equation?tex=w%5E%7B%5Bl%5D%7D" alt="公式"> 比没有正则项的值要小一些。不断迭代更新，不断地减小。</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D%20w%5E%7B%5Bl%5D%7D%20:&%20=%20w%5E%7B%5Bl%5D%7D-%5Calpha%5Ccdot%20dw%5E%7B%5Bl%5D%7D%5C%5C%20&%20=%20w%5E%7B%5Bl%5D%7D-%5Calpha%5Ccdot(dw%5E%7B%5Bl%5D%7D_%7Bbefore%7D+%5Cfrac%7B%5Clambda%7D%7Bm%7Dw%5E%7B%5Bl%5D%7D)%5C%5C%20&%20=%20(1-%5Calpha%5Cfrac%7B%5Clambda%7D%7Bm%7D)w%5E%7B%5Bl%5D%7D-%5Calpha%5Ccdot%20dw%5E%7B%5Bl%5D%7D_%7Bbefore%7D%20%5Cend%7Baligned%7D" alt="公式"></p>
<p>其中，<img src="https://www.zhihu.com/equation?tex=(1-%5Calpha%5Cfrac%7B%5Clambda%7D%7Bm%7D)%3C1" alt="公式"> 。</p>
<h3 id="3-4-正则化可以减小过拟合的原因"><a href="#3-4-正则化可以减小过拟合的原因" class="headerlink" title="3.4 正则化可以减小过拟合的原因"></a>3.4 正则化可以减小过拟合的原因</h3><h4 id="1-直观解释"><a href="#1-直观解释" class="headerlink" title="(1) 直观解释"></a>(1) 直观解释</h4><p>我们回到上面模型状态的那张图，从左到右，分别表示了<strong>欠拟合</strong>、<strong>刚好拟合</strong>、<strong>过拟合</strong>三种情况。选择图中的复杂神经网络模型，那么不添加正则化的情况下，我们可能得到图中的过拟合分类边界。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/f2f0b0eb9093743b8831799bb8b71314.png" alt="神经网络的正则化"></p>
<p>如果使用L2正则化，当 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="公式"> 很大时，<img src="https://www.zhihu.com/equation?tex=w%5E%7B%5Bl%5D%7D%5Capprox0" alt="公式"> 即 <img src="https://www.zhihu.com/equation?tex=w%5E%7B%5Bl%5D%7D" alt="公式"> 近似为零，意味着该神经网络模型中的某些神经元实际的作用很小，可以忽略。从效果上来看，其实是将某些神经元给忽略掉了。这样原本过于复杂的神经网络模型就变得不那么复杂了，而变得非常简单化了。</p>
<p>如图所示，整个简化的神经网络模型变成了一个逻辑回归模型。<strong>问题就从High Variance变成了High Bias了</strong>。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/e4258e08d75c71d2fec898e9acbcc76c.png" alt="神经网络的正则化"></p>
<p>因此，总结一下，直观的一种理解是：正则化因子设置的足够大的情况下，为了使成本函数最小化，权重矩阵 <img src="https://www.zhihu.com/equation?tex=W" alt="公式"> 就会被设置为接近于0的值，直观上相当于消除了很多神经元的影响，那么大的神经网络就会变成一个较小的网络。当然，实际上隐藏层的神经元依然存在，但是其影响减弱了，过拟合可能性大大减小。</p>
<h4 id="2-数学解释"><a href="#2-数学解释" class="headerlink" title="(2) 数学解释"></a>(2) 数学解释</h4><p>假设神经元中使用的激活函数为 <img src="https://www.zhihu.com/equation?tex=g(z)%20=%20tanh(z)" alt="公式"> (sigmoid同理) 。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/0a6a8c049dcb3ac6d9272879062f7cd6.png" alt="神经网络的正则化"></p>
<p>在加入正则化项后，当 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="公式"> 增大，导致 <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="公式"> 减小，<img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D%20=%20W%5E%7B%5Bl%5D%7Da%5E%7B%5Bl-1%5D%7D%20+%20b%5E%7B%5Bl%5D%7D" alt="公式"> 便会减小。通过上图我们会发现，在 <img src="https://www.zhihu.com/equation?tex=z" alt="公式"> 较小 (接近于0) 的区域里，<img src="https://www.zhihu.com/equation?tex=tanh(z)" alt="公式"> 函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，因此不会发生过拟合。</p>
<h4 id="3-其他解释"><a href="#3-其他解释" class="headerlink" title="(3) 其他解释"></a>(3) 其他解释</h4><p>在权值 <img src="https://www.zhihu.com/equation?tex=w%5E%7B%5BL%5D%7D" alt="公式"> 变小之下，输入样本 <img src="https://www.zhihu.com/equation?tex=X" alt="公式"> 随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。</p>
<h2 id="4-Dropout-正则化"><a href="#4-Dropout-正则化" class="headerlink" title="4.Dropout 正则化"></a>4.Dropout 正则化</h2><p>在神经网络中，另外一种很有效的正则化方式叫做Dropout (随机失活) ，它是指在神经网络的隐藏层为每个神经元结点设置一个随机关闭的概率，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。网络模型得到简化，从而避免发生过拟合。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/4b98f5e6e6a0f3dd94bd6514c9f7696c.png" alt="Dropout正则化"></p>
<h3 id="4-1-反向随机失活-Inverted-Dropout"><a href="#4-1-反向随机失活-Inverted-Dropout" class="headerlink" title="4.1 反向随机失活 (Inverted Dropout)"></a>4.1 反向随机失活 (Inverted Dropout)</h3><p>Dropout有不同的实现方法，一种常用的方法是<strong>Inverted Dropout</strong>。假设对于第l层神经元，设定保留神经元比例概率keep_prob&#x3D;0.8，即该层有20%的神经元停止工作。<strong>dl</strong> 为Dropout向量，设置 <strong>dl</strong>为随机vector，其中80%的元素为1，20%的元素为0。</p>
<p><strong>Dropout vector的生成python代码如下所示</strong>：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/d1e0b6f58a2f6c17e72b04cf076d7d2b.png" alt="Dropout正则化"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keep_prob = 0.8  # 设置神经元保留概率dl = np.random.rand (al.shape[0], al.shape[1]) &lt; keep_probal = np.multiply (al, dl)al /= keep_prob</span><br></pre></td></tr></table></figure>

<ul>
<li>最后一步<code>al /= keep_prob</code>是因为 <img src="https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D" alt="公式"> 中的一部分元素失活 (相当于被归零) ，为了在下一层计算时不影响 <img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl+1%5D%7D%20=%20W%5E%7B%5Bl+1%5D%7Da%5E%7B%5Bl%5D%7D%20+%20b%5E%7B%5Bl+1%5D%7D" alt="公式"> 的期望值，因此除以一个<code>keep_prob</code>。</li>
</ul>
<p>Inverted Dropout 的另外一个好处就是在对该 Dropout 后的神经网络进行测试时能够减少 scaling 问题。因为在训练时，使用 scale up 保证 al 的期望值没有大的变化，测试时就不需要再对样本数据进行类似的尺度伸缩操作了。</p>
<ul>
<li>对于 <img src="https://www.zhihu.com/equation?tex=m" alt="公式"> 个样本，单次迭代训练时，随机删除掉隐藏层一定数量的神经元；</li>
<li>然后，在删除后的剩下的神经元上正向和反向更新权重 <img src="https://www.zhihu.com/equation?tex=w" alt="公式"> 和常数项 <img src="https://www.zhihu.com/equation?tex=b" alt="公式"> ；</li>
<li>接着，下一次迭代中，再恢复之前删除的神经元，重新随机删除一定数量的神经元，进行正向和反向更新 <img src="https://www.zhihu.com/equation?tex=w" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=b" alt="公式"> 。</li>
<li>不断重复上述过程，直至迭代训练完成。</li>
</ul>
<p><strong>注意</strong>：使用Dropout训练结束后，在测试和实际应用模型时，不需要进行Dropout和随机删减神经元，所有的神经元都在工作。</p>
<h3 id="4-2-理解-Dropout"><a href="#4-2-理解-Dropout" class="headerlink" title="4.2 理解 Dropout"></a>4.2 理解 Dropout</h3><h4 id="4-2-1-Dropout理解视角1"><a href="#4-2-1-Dropout理解视角1" class="headerlink" title="4.2.1 Dropout理解视角1"></a>4.2.1 Dropout理解视角1</h4><p>Dropout通过每次迭代训练时，随机选择不同的神经元，相当于每次都在不同的神经网络上进行训练，类似机器学习中Bagging的方法  ，能够防止过拟合。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/6158c68b161eeaac6798855e68661dc2.png" alt="Dropout正则化"></p>
<h4 id="4-2-2-Dropout理解视角2"><a href="#4-2-2-Dropout理解视角2" class="headerlink" title="4.2.2 Dropout理解视角2"></a>4.2.2 Dropout理解视角2</h4><p>第2个理解的视角是Dropout会减小权重 <img src="https://www.zhihu.com/equation?tex=w" alt="公式"> 的值。</p>
<p>对于某个神经元来说，某次训练时，它的某些输入在Dropout的作用被过滤了。而在下一次训练时，又有不同的某些输入被过滤。经过多次训练后，某些输入被过滤，某些输入被保留。这样，神经元不会再特别依赖于任何一个输入特征。也就是说，对应的权重 <img src="https://www.zhihu.com/equation?tex=w" alt="公式"> 不会很大。<strong>这从效果上来说，与L2 正则化是类似的，都是对权重 <img src="https://www.zhihu.com/equation?tex=w" alt="公式"> 进行「惩罚」，减小了 <img src="https://www.zhihu.com/equation?tex=w" alt="公式"> 的值</strong>。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/80521020b9fc659dc904f57b8bedf3b2.png" alt="Dropout正则化"></p>
<p>因此，通过传播过程，Dropout将产生和 <img src="https://www.zhihu.com/equation?tex=L2" alt="公式"> 正则化相同的收缩权重的效果。</p>
<h3 id="4-3-Dropout取值"><a href="#4-3-Dropout取值" class="headerlink" title="4.3 Dropout取值"></a>4.3 Dropout取值</h3><p>一般来说，神经元多的隐藏层，<code>keep_prob</code>可以设置得小一些，例如0.5；神经元越少的隐藏层，<code>keep_out</code>可以设置的大一些，例如0.8，设置是1。</p>
<p>实际应用中，不建议对输入层进行Dropout，如果输入层维度很大，例如图片，那么可以设置Dropout，但<code>keep_prob</code>应设置的大一些，例如0.8，0.9。</p>
<p>总体来说，就是越容易出现overfitting的隐藏层，其<code>keep_prob</code>就设置的相对小一些。没有准确固定的做法，通常可以根据validation进行选择。</p>
<p><strong>注意</strong>：Dropout的一大缺点是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用Dropout时，先将<code>keep_prob</code>全部设置为1.0后运行代码，确保 <img src="https://www.zhihu.com/equation?tex=J(w,%20b)" alt="公式"> 函数单调递减，再打开Dropout。</p>
<h2 id="5-其他正则化方法"><a href="#5-其他正则化方法" class="headerlink" title="5.其他正则化方法"></a>5.其他正则化方法</h2><h3 id="5-1-数据扩增-Data-Augmentation"><a href="#5-1-数据扩增-Data-Augmentation" class="headerlink" title="5.1 数据扩增 (Data Augmentation)"></a>5.1 数据扩增 (Data Augmentation)</h3><p>数据扩增 (Data Augmentation) 是深度学习中常见和有效的技巧，特别的，在计算机视觉领域，它指的<strong>通过图片的一些变换 (翻转，局部放大后切割等) ，得到更多的训练集和验证集</strong>。如下图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/e4f6c33b494415bf800bb9366b747964.png" alt="其他正则化方法"></p>
<h3 id="5-2-早停止法-Early-Stopping"><a href="#5-2-早停止法-Early-Stopping" class="headerlink" title="5.2 早停止法 (Early Stopping)"></a>5.2 早停止法 (Early Stopping)</h3><p>因为深度学习的训练过程是一个不断迭代优化训练集cost function的过程，但是迭代次数过多会导致模型过度拟合训练集而对其他数据泛化能力变弱。一个处理方法是使用早停止法 (Early Stopping) 。</p>
<p>在早停止法 (Early Stopping) 中，我们会把训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内。当训练集误差降低但验证集误差升高，两者开始发生较大偏差时及时停止迭代，并返回具有最小验证集误差的连接权和阈值，以避免过拟合。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/f798556a5c3a56c20c16f976a4f58ff9.png" alt="其他正则化方法"></p>
<p><strong>Early Stopping也有其自身缺点</strong>。</p>
<p>回顾我们应用机器学习训练模型有两个目标：① 优化Cost Function，尽量减小 <img src="https://www.zhihu.com/equation?tex=J" alt="公式"> ；② 防止过拟合，希望在新数据上有好的泛化能力。这两个目标彼此对立的，即减小 <img src="https://www.zhihu.com/equation?tex=J" alt="公式"> 的同时可能会造成过拟合，反之亦然。</p>
<p>前面提到过，在深度学习中，神经网络可以同时减小Bias和Variance，构建最佳模型。但是，Early Stopping的做法通过减少得带训练次数来防止过拟合，这样 <img src="https://www.zhihu.com/equation?tex=J" alt="公式"> 就不会足够小。也就是说，<strong>Early Stopping将上述两个目标融合在一起，同时优化，但可能没有「分而治之」的效果好</strong>。</p>
<p><strong>对比Early Stopping，L2正则化可以实现「分而治之」的效果：迭代训练足够多，减小 <img src="https://www.zhihu.com/equation?tex=J" alt="公式">，而且也能有效防止过拟合。而L2正则化的缺点之一是最优的正则化参数 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="公式"> 的选择比较复杂，这点上Early Stopping比较简单</strong>。</p>
<p>总体上 <img src="https://www.zhihu.com/equation?tex=L2" alt="公式"> 正则化更加常用一些。</p>
<h2 id="6-标准化输入"><a href="#6-标准化输入" class="headerlink" title="6.标准化输入"></a>6.标准化输入</h2><h3 id="6-1-标准化输入操作"><a href="#6-1-标准化输入操作" class="headerlink" title="6.1 标准化输入操作"></a>6.1 标准化输入操作</h3><p>在训练神经网络时，对输入标准化可以提高训练的速度。标准化就是对训练数据集进行归一化的操作，即将原始数据减去其均值 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="公式"> 后，再除以其方差 <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2" alt="公式"> ：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmu=%5Cfrac1m%5Csum_%7Bi=1%7D%5EmX%5E%7B(i)%7D" alt="公式"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2=%5Cfrac1m%5Csum_%7Bi=1%7D%5Em(X%5E%7B(i)%7D)%5E2" alt="公式"></p>
<p><img src="https://www.zhihu.com/equation?tex=X:=%5Cfrac%7BX-%5Cmu%7D%7B%5Csigma%5E2%7D" alt="公式"></p>
<p>下图展示二维数据的归一化过程及其分布变化：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/afe49db5fe17fe884ddd13652a0f0a94.png" alt="标准化输入"></p>
<p><strong>注意</strong>：实际建模应用时，对于测试集，应该使用训练集同样的 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2" alt="公式"> 对其进行标准化处理。这样保证了训练集和测试集的标准化操作一致。</p>
<h3 id="6-2-标准化输入原因"><a href="#6-2-标准化输入原因" class="headerlink" title="6.2 标准化输入原因"></a>6.2 标准化输入原因</h3><p>标准化输入可以让所有输入调整到同样的尺度scale上，方便进行梯度下降算法时能够更快更准确地找到全局最优解。</p>
<p>以二维数据为例，如果输入数据有 <img src="https://www.zhihu.com/equation?tex=x_1" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=x_2" alt="公式"> 两个维度，<img src="https://www.zhihu.com/equation?tex=x_1" alt="公式"> 的范围是 <img src="https://www.zhihu.com/equation?tex=%5B1,1000%5D" alt="公式">，<img src="https://www.zhihu.com/equation?tex=x_2" alt="公式"> 的范围是 <img src="https://www.zhihu.com/equation?tex=%5B0,1%5D" alt="公式">。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/86829f849204d8c2c64eeb629e95439a.png" alt="标准化输入"></p>
<h4 id="1-未做标准化的情形"><a href="#1-未做标准化的情形" class="headerlink" title="(1) 未做标准化的情形"></a>(1) 未做标准化的情形</h4><p>不进行标准化处理的情况下，<img src="https://www.zhihu.com/equation?tex=x_1" alt="公式"> 与 <img src="https://www.zhihu.com/equation?tex=x_2" alt="公式"> 之间分布极不平衡，训练得到的 <img src="https://www.zhihu.com/equation?tex=w_1" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=w_2" alt="公式"> 也会在数量级上差别很大。这种情形下的Cost Function与 <img src="https://www.zhihu.com/equation?tex=w" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=b" alt="公式"> 的关系可能是一个非常细长的椭圆形碗，如图左所示。</p>
<p>对这种Cost Function进行梯度下降优化时，由于 <img src="https://www.zhihu.com/equation?tex=w_1" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=w_2" alt="公式"> 数值差异很大，只能选择很小的学习因子 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="公式">，来避免 <img src="https://www.zhihu.com/equation?tex=J" alt="公式"> 发生振荡。一旦 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="公式"> 较大，必然发生振荡，<img src="https://www.zhihu.com/equation?tex=J" alt="公式"> 不再单调下降。</p>
<h4 id="2-做完标准化的情形"><a href="#2-做完标准化的情形" class="headerlink" title="(2) 做完标准化的情形"></a>(2) 做完标准化的情形</h4><p>如果进行了标准化操作，<img src="https://www.zhihu.com/equation?tex=x_1" alt="公式"> 与 <img src="https://www.zhihu.com/equation?tex=x_2" alt="公式"> 分布均匀，<img src="https://www.zhihu.com/equation?tex=w_1" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=w_2" alt="公式"> 数值差别不大，得到的Cost Function与 <img src="https://www.zhihu.com/equation?tex=w" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=b" alt="公式"> 的关系是类似圆形碗，如右图所示。对其进行梯度下降优化时，<img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="公式"> 可以选择相对大一些，且 <img src="https://www.zhihu.com/equation?tex=J" alt="公式"> 一般不会发生振荡，保证了 <img src="https://www.zhihu.com/equation?tex=J" alt="公式"> 是单调下降的。</p>
<p>如果输入特征之间的范围本来就比较接近，那么不进行标准化操作也是没有太大影响的。但是，标准化处理在大多数场合下还是值得做的。</p>
<h2 id="7-梯度消失和梯度爆炸"><a href="#7-梯度消失和梯度爆炸" class="headerlink" title="7.梯度消失和梯度爆炸"></a>7.梯度消失和梯度爆炸</h2><h3 id="7-1-梯度爆炸与梯度消失"><a href="#7-1-梯度爆炸与梯度消失" class="headerlink" title="7.1 梯度爆炸与梯度消失"></a>7.1 梯度爆炸与梯度消失</h3><p>在深度神经网络里，我们在计算损失函数梯度时，有时会出现以指数级递增或者递减的情况，它们分别对应神经网络的梯度爆炸和梯度消失问题。</p>
<p>举个例子来说明，假设一个多层的每层只包含两个神经元的深度神经网络模型，如下图所示：</p>
<p>为了简化复杂度，便于分析，我们令各层的激活函数为线性函数，且忽略各层常数项 <img src="https://www.zhihu.com/equation?tex=b" alt="公式"> 的影响，即假定 <img src="https://www.zhihu.com/equation?tex=g(z)%20=%20z" alt="公式">，<img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D%20=%200" alt="公式">，对于目标输出 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D" alt="公式"> 有：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/19bcef759195d6b081a4ae9ffd5dc2a5.png" alt="梯度消失和梯度爆炸"></p>
<p>这种叠乘会带来下面2种情况：</p>
<ul>
<li>对于 <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="公式"> 的值大于1的情况，激活函数的值将以指数级递增；</li>
<li>对于 <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="公式"> 的值小于1的情况，激活函数的值将以指数级递减。</li>
</ul>
<p>计算梯度是一个类似的过程，根据求导链式法则，也会有叠乘情况出现，梯度函数会以指数级递增或递减，导致训练导数难度上升，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。</p>
<h3 id="7-2-权重初始化缓解梯度消失和爆炸"><a href="#7-2-权重初始化缓解梯度消失和爆炸" class="headerlink" title="7.2 权重初始化缓解梯度消失和爆炸"></a>7.2 权重初始化缓解梯度消失和爆炸</h3><p>那么怎么改善梯度消失和爆炸问题呢？一种方法是对权重 <img src="https://www.zhihu.com/equation?tex=w" alt="公式"> 进行一些初始化处理。</p>
<p>深度神经网络模型中，以单个神经元为例，其输出计算为 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D" alt="公式"> ：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/dd21ac81e1f28c17a943096a332cd8a7.png" alt="梯度消失和梯度爆炸"></p>
<p>为了让 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D" alt="公式"> 不会过大或者过小，思路是让 <img src="https://www.zhihu.com/equation?tex=w" alt="公式"> 与 <img src="https://www.zhihu.com/equation?tex=n" alt="公式"> 有关，且 <img src="https://www.zhihu.com/equation?tex=n" alt="公式"> 越大，<img src="https://www.zhihu.com/equation?tex=w" alt="公式"> 应该越小才好。一种方法是在初始化w时，令其方差为 <img src="https://www.zhihu.com/equation?tex=1/n" alt="公式">，这里称为<strong>Xavier initialization</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">## 针对tanh激活函数的Xavier初始化WL = np.random.randn (WL.shape[0], WL.shape[1]) * np.sqrt (1/n)</span><br></pre></td></tr></table></figure>

<p>其中 <img src="https://www.zhihu.com/equation?tex=n" alt="公式"> 是输入的神经元个数，即 <code>WL.shape[1]</code>。</p>
<p>这样，激活函数的输入 <img src="https://www.zhihu.com/equation?tex=x" alt="公式"> 近似设置成均值为0，标准方差为1，神经元输出 <img src="https://www.zhihu.com/equation?tex=z" alt="公式"> 的方差就正则化到1了。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。</p>
<p>如果使用的是ReLU激活函数，权重 <img src="https://www.zhihu.com/equation?tex=w" alt="公式"> 的初始化一般令其方差为 <img src="https://www.zhihu.com/equation?tex=2/n" alt="公式"> 对应 Python 代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l-1])*np.sqrt(2/n[l-1])</span><br></pre></td></tr></table></figure>

<h2 id="8-梯度检验-Gradient-checking"><a href="#8-梯度检验-Gradient-checking" class="headerlink" title="8.梯度检验 (Gradient checking)"></a>8.梯度检验 (Gradient checking)</h2><h3 id="8-1-梯度的数值逼近"><a href="#8-1-梯度的数值逼近" class="headerlink" title="8.1 梯度的数值逼近"></a>8.1 梯度的数值逼近</h3><p>我们知道梯度下降法会大程度依赖梯度来完成，在数学上，我们可以基于微分的定义，使用极限的计算去逼近导数，我们有如下的「<strong>单边误差法</strong>」和「<strong>双边误差法</strong>」，其中后者精度要高一些。</p>
<h4 id="1-单边误差"><a href="#1-单边误差" class="headerlink" title="(1) 单边误差"></a>(1) 单边误差</h4><p><img src="https://img-blog.csdnimg.cn/img_convert/81833856e91a598bbb404056f5f800a5.png" alt="梯度检验"></p>
<h4 id="2-双边误差求导-即导数的定义"><a href="#2-双边误差求导-即导数的定义" class="headerlink" title="(2) 双边误差求导 (即导数的定义)"></a>(2) 双边误差求导 (即导数的定义)</h4><p><img src="https://img-blog.csdnimg.cn/img_convert/fa8ef333db58212e7656fe6634bcaa54.png" alt="梯度检验"></p>
<p>当 <img src="https://www.zhihu.com/equation?tex=%5Cvarepsilon" alt="公式"> 越小时，结果越接近真实的导数，也就是梯度值。可以使用这种方法来判断反向传播进行梯度下降时，是否出现了错误。</p>
<h2 id="8-2-梯度检验"><a href="#8-2-梯度检验" class="headerlink" title="8.2 梯度检验"></a>8.2 梯度检验</h2><p><img src="https://img-blog.csdnimg.cn/img_convert/df6bc97d7b89467098498e6be29a1e78.png" alt="梯度检验 Gradient Checking"></p>
<p>当我们计算出数值梯度后，要进行梯度检查，来验证训练过程中是否有问题。</p>
<h4 id="1-连接参数"><a href="#1-连接参数" class="headerlink" title="(1) 连接参数"></a>(1) 连接参数</h4><p>将 <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5B1%5D%7D" alt="公式">，<img src="https://www.zhihu.com/equation?tex=b%5E%7B%5B1%5D%7D" alt="公式">，…，<img src="https://www.zhihu.com/equation?tex=W%5E%7B%5BL%5D%7D" alt="公式">，<img src="https://www.zhihu.com/equation?tex=b%5E%7B%5BL%5D%7D" alt="公式"> 全部连接出来，成为一个巨型向量 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="公式">。</p>
<p>同时，对 <img src="https://www.zhihu.com/equation?tex=dW%5E%7B%5B1%5D%7D" alt="公式">，<img src="https://www.zhihu.com/equation?tex=db%5E%7B%5B1%5D%7D" alt="公式">，…，<img src="https://www.zhihu.com/equation?tex=dW%5E%7B%5BL%5D%7D" alt="公式">，<img src="https://www.zhihu.com/equation?tex=db%5E%7B%5BL%5D%7D" alt="公式"> 执行同样的操作得到巨型向量 <img src="https://www.zhihu.com/equation?tex=d%5Ctheta" alt="公式">，它和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="公式"> 有同样的维度。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/24e05cc98bf10bdb1beef77247ea6032.png" alt="连接参数"></p>
<p>现在，我们需要找到 <img src="https://www.zhihu.com/equation?tex=d%5Ctheta" alt="公式"> 和代价函数 <img src="https://www.zhihu.com/equation?tex=J" alt="公式"> 的梯度的关系。</p>
<h4 id="2-进行梯度检验"><a href="#2-进行梯度检验" class="headerlink" title="(2) 进行梯度检验"></a>(2) 进行梯度检验</h4><p>求得一个梯度逼近值 <img src="https://www.zhihu.com/equation?tex=d%5Ctheta_%7Bapprox%7D%5Bi%5D" alt="公式">，应该 <img src="https://www.zhihu.com/equation?tex=%5Capprox%20d%5Ctheta%5Bi%5D%20=%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20%5Ctheta_i%7D" alt="公式"> 。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/d5e682a6896731de7e5b716972732f76.png" alt="梯度检验"></p>
<p>因此，我们用梯度检验值检验反向传播的实施是否正确。其中，<img src="https://www.zhihu.com/equation?tex=%7B%7C%7Cx%7C%7C%7D_2" alt="公式"> 表示向量 <img src="https://www.zhihu.com/equation?tex=x" alt="公式"> 的<strong>2-范数</strong> (也称「<strong>欧几里德范数</strong>」) 。</p>
<p><strong>如果梯度检验值和 <img src="https://www.zhihu.com/equation?tex=%5Cvarepsilon" alt="公式"> 的值相近，说明神经网络的实施是正确的，否则要去检查代码是否存在bug</strong>。</p>
<h3 id="8-3在神经网络实施梯度检验的实用技巧和注意事项"><a href="#8-3在神经网络实施梯度检验的实用技巧和注意事项" class="headerlink" title="8.3在神经网络实施梯度检验的实用技巧和注意事项"></a>8.3在神经网络实施梯度检验的实用技巧和注意事项</h3><p>在进行梯度检查的过程中有几点需要注意的地方：</p>
<ul>
<li>不要在整个训练过程中使用梯度检验，它仅仅用于调试。</li>
<li>如果算法的梯度检验失败，找到对应出错的梯度 (即 <img src="https://www.zhihu.com/equation?tex=d%5Ctheta%20%5Capprox%20%5Bi%5D" alt="公式"> 与 <img src="https://www.zhihu.com/equation?tex=d%5Ctheta" alt="公式"> 的值相差比较大的项) ，检查其推导是否出现错误。</li>
<li>计算近似梯度时，要带上正则项。</li>
<li>梯度检查时关闭dropout，检查完毕后再打开dropout。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://olstussy.top">Olstussy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://olstussy.top/2023/08/03/5.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8/">https://olstussy.top/2023/08/03/5.神经网络应用/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://olstussy.top" target="_blank">Olstussy</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2023/08/02/UOWRIZPuaAX81yL.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/08/03/4.%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="AI笔记（4）：深层神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">AI笔记（4）：深层神经网络</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/08/03/3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="AI笔记（3）：浅层神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-03</div><div class="title">AI笔记（3）：浅层神经网络</div></div></a></div><div><a href="/2023/08/02/2.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" title="AI笔记（2）：神经网络基础"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-02</div><div class="title">AI笔记（2）：神经网络基础</div></div></a></div><div><a href="/2023/08/03/4.%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="AI笔记（4）：深层神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-03</div><div class="title">AI笔记（4）：深层神经网络</div></div></a></div><div><a href="/2023/08/01/1.%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86,%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/" title="AI笔记（1）：基础知识"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-01</div><div class="title">AI笔记（1）：基础知识</div></div></a></div><div><a href="/2023/08/02/numpy%E7%AC%94%E8%AE%B0/" title="Numpy笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-02</div><div class="title">Numpy笔记</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2023/08/02/UOWRIZPuaAX81yL.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Olstussy</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/olstussy"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">No Error No Warning !</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E5%88%92%E5%88%86%EF%BC%9A%E8%AE%AD%E7%BB%83-%E9%AA%8C%E8%AF%81-%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-number">1.</span> <span class="toc-text">1.数据划分：训练 &#x2F; 验证 &#x2F; 测试集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E8%BF%AD%E4%BB%A3%E4%BC%98%E5%8C%96"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 深度学习实践迭代优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E5%89%8D%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E4%BB%A3%E5%88%92%E5%88%86%E6%96%B9%E5%BC%8F"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 前大数据时代划分方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E4%BB%A3%E5%88%92%E5%88%86%E6%96%B9%E5%BC%8F"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 大数据时代划分方式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="toc-number">1.3.1.</span> <span class="toc-text">关于验证集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-number">1.3.2.</span> <span class="toc-text">关于测试集</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E6%95%B0%E6%8D%AE%E5%88%92%E5%88%86%E5%BB%BA%E8%AE%AE"><span class="toc-number">1.4.</span> <span class="toc-text">1.4 数据划分建议</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%A8%A1%E5%9E%8B%E4%BC%B0%E8%AE%A1%EF%BC%9A%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE"><span class="toc-number">2.</span> <span class="toc-text">2.模型估计：偏差 &#x2F; 方差</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%A8%A1%E5%9E%8B%E7%8A%B6%E6%80%81%E4%B8%8E%E8%AF%84%E4%BC%B0"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 模型状态与评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%BA%94%E5%AF%B9%E6%96%B9%E6%B3%95"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 应对方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%AD%A3%E5%88%99%E5%8C%96-Regularization"><span class="toc-number">3.</span> <span class="toc-text">3.正则化 (Regularization)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Logistic-%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 Logistic 回归中的正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 神经网络中的正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E6%AD%A3%E5%88%99%E5%8C%96%E5%8F%AF%E4%BB%A5%E5%87%8F%E5%B0%8F%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E5%8E%9F%E5%9B%A0"><span class="toc-number">3.4.</span> <span class="toc-text">3.4 正则化可以减小过拟合的原因</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%9B%B4%E8%A7%82%E8%A7%A3%E9%87%8A"><span class="toc-number">3.4.1.</span> <span class="toc-text">(1) 直观解释</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%95%B0%E5%AD%A6%E8%A7%A3%E9%87%8A"><span class="toc-number">3.4.2.</span> <span class="toc-text">(2) 数学解释</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%85%B6%E4%BB%96%E8%A7%A3%E9%87%8A"><span class="toc-number">3.4.3.</span> <span class="toc-text">(3) 其他解释</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Dropout-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">4.</span> <span class="toc-text">4.Dropout 正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%8F%8D%E5%90%91%E9%9A%8F%E6%9C%BA%E5%A4%B1%E6%B4%BB-Inverted-Dropout"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 反向随机失活 (Inverted Dropout)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E7%90%86%E8%A7%A3-Dropout"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 理解 Dropout</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1-Dropout%E7%90%86%E8%A7%A3%E8%A7%86%E8%A7%921"><span class="toc-number">4.2.1.</span> <span class="toc-text">4.2.1 Dropout理解视角1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-Dropout%E7%90%86%E8%A7%A3%E8%A7%86%E8%A7%922"><span class="toc-number">4.2.2.</span> <span class="toc-text">4.2.2 Dropout理解视角2</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-Dropout%E5%8F%96%E5%80%BC"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 Dropout取值</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%85%B6%E4%BB%96%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">5.其他正则化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E-Data-Augmentation"><span class="toc-number">5.1.</span> <span class="toc-text">5.1 数据扩增 (Data Augmentation)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E6%97%A9%E5%81%9C%E6%AD%A2%E6%B3%95-Early-Stopping"><span class="toc-number">5.2.</span> <span class="toc-text">5.2 早停止法 (Early Stopping)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E6%A0%87%E5%87%86%E5%8C%96%E8%BE%93%E5%85%A5"><span class="toc-number">6.</span> <span class="toc-text">6.标准化输入</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E6%A0%87%E5%87%86%E5%8C%96%E8%BE%93%E5%85%A5%E6%93%8D%E4%BD%9C"><span class="toc-number">6.1.</span> <span class="toc-text">6.1 标准化输入操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E6%A0%87%E5%87%86%E5%8C%96%E8%BE%93%E5%85%A5%E5%8E%9F%E5%9B%A0"><span class="toc-number">6.2.</span> <span class="toc-text">6.2 标准化输入原因</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%9C%AA%E5%81%9A%E6%A0%87%E5%87%86%E5%8C%96%E7%9A%84%E6%83%85%E5%BD%A2"><span class="toc-number">6.2.1.</span> <span class="toc-text">(1) 未做标准化的情形</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%81%9A%E5%AE%8C%E6%A0%87%E5%87%86%E5%8C%96%E7%9A%84%E6%83%85%E5%BD%A2"><span class="toc-number">6.2.2.</span> <span class="toc-text">(2) 做完标准化的情形</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="toc-number">7.</span> <span class="toc-text">7.梯度消失和梯度爆炸</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E4%B8%8E%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="toc-number">7.1.</span> <span class="toc-text">7.1 梯度爆炸与梯度消失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E7%BC%93%E8%A7%A3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E7%88%86%E7%82%B8"><span class="toc-number">7.2.</span> <span class="toc-text">7.2 权重初始化缓解梯度消失和爆炸</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C-Gradient-checking"><span class="toc-number">8.</span> <span class="toc-text">8.梯度检验 (Gradient checking)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%95%B0%E5%80%BC%E9%80%BC%E8%BF%91"><span class="toc-number">8.1.</span> <span class="toc-text">8.1 梯度的数值逼近</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%8D%95%E8%BE%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">8.1.1.</span> <span class="toc-text">(1) 单边误差</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%8F%8C%E8%BE%B9%E8%AF%AF%E5%B7%AE%E6%B1%82%E5%AF%BC-%E5%8D%B3%E5%AF%BC%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-number">8.1.2.</span> <span class="toc-text">(2) 双边误差求导 (即导数的定义)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C"><span class="toc-number">9.</span> <span class="toc-text">8.2 梯度检验</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%BF%9E%E6%8E%A5%E5%8F%82%E6%95%B0"><span class="toc-number">9.0.1.</span> <span class="toc-text">(1) 连接参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E8%BF%9B%E8%A1%8C%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C"><span class="toc-number">9.0.2.</span> <span class="toc-text">(2) 进行梯度检验</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3%E5%9C%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%96%BD%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%E7%9A%84%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7%E5%92%8C%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-number">9.1.</span> <span class="toc-text">8.3在神经网络实施梯度检验的实用技巧和注意事项</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/08/03/5.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8/" title="AI笔记（5）：神经网络应用">AI笔记（5）：神经网络应用</a><time datetime="2023-08-03T10:41:07.000Z" title="发表于 2023-08-03 18:41:07">2023-08-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/08/03/4.%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="AI笔记（4）：深层神经网络">AI笔记（4）：深层神经网络</a><time datetime="2023-08-03T07:41:07.000Z" title="发表于 2023-08-03 15:41:07">2023-08-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/08/03/3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="AI笔记（3）：浅层神经网络">AI笔记（3）：浅层神经网络</a><time datetime="2023-08-03T05:41:07.000Z" title="发表于 2023-08-03 13:41:07">2023-08-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/08/02/shell%E7%AC%94%E8%AE%B0/" title="Shell笔记">Shell笔记</a><time datetime="2023-08-02T12:40:21.000Z" title="发表于 2023-08-02 20:40:21">2023-08-02</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/08/02/2.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" title="AI笔记（2）：神经网络基础">AI笔记（2）：神经网络基础</a><time datetime="2023-08-02T04:41:07.000Z" title="发表于 2023-08-02 12:41:07">2023-08-02</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By Olstussy</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>