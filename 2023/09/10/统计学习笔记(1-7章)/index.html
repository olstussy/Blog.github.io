<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>统计学习笔记(1-7章) | Olstussy</title><meta name="author" content="Olstussy"><meta name="copyright" content="Olstussy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="基础知识：  先验概率和后验概率:   先验概率就是事情发生前的预测概率. 后验概率是一种条件概率，它限定了事件为隐变量取值，而条件为观测结果。一般的条件概率，条件和事件可以是任意的. 贝叶斯公式P(y|x) &#x3D; ( P(x|y) * P(y) ) &#x2F; P(x)中,P(y|x)是后验概率,P(x|y)是条件概率,P(y)是先验概率.   判别式模型和生成式模型:   判别式模">
<meta property="og:type" content="article">
<meta property="og:title" content="统计学习笔记(1-7章)">
<meta property="og:url" content="https://olstussy.top/2023/09/10/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1-7%E7%AB%A0)/index.html">
<meta property="og:site_name" content="Olstussy">
<meta property="og:description" content="基础知识：  先验概率和后验概率:   先验概率就是事情发生前的预测概率. 后验概率是一种条件概率，它限定了事件为隐变量取值，而条件为观测结果。一般的条件概率，条件和事件可以是任意的. 贝叶斯公式P(y|x) &#x3D; ( P(x|y) * P(y) ) &#x2F; P(x)中,P(y|x)是后验概率,P(x|y)是条件概率,P(y)是先验概率.   判别式模型和生成式模型:   判别式模">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/08/02/UOWRIZPuaAX81yL.jpg">
<meta property="article:published_time" content="2023-09-10T02:36:21.000Z">
<meta property="article:modified_time" content="2023-09-10T02:48:08.035Z">
<meta property="article:author" content="Olstussy">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Numpy">
<meta property="article:tag" content="统计学">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/08/02/UOWRIZPuaAX81yL.jpg"><link rel="shortcut icon" href="/img/favicon-32x32-next.png"><link rel="canonical" href="https://olstussy.top/2023/09/10/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1-7%E7%AB%A0)/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '统计学习笔记(1-7章)',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-09-10 10:48:08'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2023/08/02/UOWRIZPuaAX81yL.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2023/08/02/n2kWORi8zyCmNXH.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Olstussy"><span class="site-name">Olstussy</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">统计学习笔记(1-7章)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-09-10T02:36:21.000Z" title="发表于 2023-09-10 10:36:21">2023-09-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-09-10T02:48:08.035Z" title="更新于 2023-09-10 10:48:08">2023-09-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/">统计学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="统计学习笔记(1-7章)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p> <strong>基础知识：</strong></p>
<ul>
<li><strong>先验概率和后验概率</strong>:</li>
</ul>
<ol>
<li>先验概率就是事情发生前的预测概率.</li>
<li>后验概率是一种条件概率，它限定了事件为隐变量取值，而条件为观测结果。一般的条件概率，条件和事件可以是任意的.</li>
<li>贝叶斯公式P(y|x) &#x3D; ( P(x|y) * P(y) ) &#x2F; P(x)中,P(y|x)是后验概率,P(x|y)是条件概率,P(y)是先验概率.</li>
</ol>
<ul>
<li><strong>判别式模型和生成式模型</strong>:</li>
</ul>
<ol>
<li>判别式模型直接学习**决策函数f(X)<strong>或</strong>条件概率分布P(Y|X)**作为预测的模型.往往准确率更高,并且可以简化学习问题.如k近邻法&#x2F;感知机&#x2F;决策树&#x2F;最大熵模型&#x2F;Logistic回归&#x2F;线性判别分析(LDA)&#x2F;支持向量机(SVM)&#x2F;Boosting&#x2F;条件随机场算法(CRF)&#x2F;线性回归&#x2F;神经网络</li>
<li>生成式模型由数据学习<strong>联合概率分布P(X,Y)</strong>,然后由P(Y|X)&#x3D;P(X,Y)&#x2F;P(X)求出条件概率分布作为预测的模型,即生成模型.当存在隐变量时只能用生成方法学习.如混合高斯模型和其他混合模型&#x2F;隐马尔可夫模型(HMM)&#x2F;朴素贝叶斯&#x2F;依赖贝叶斯(AODE)&#x2F;LDA文档主题生成模型</li>
</ol>
<ul>
<li><strong>损失函数和风险函数</strong>:</li>
</ul>
<ol>
<li>损失函数度量模型一次预测的好坏.常用的损失函数有:0-1损失函数,平方损失函数,绝对损失函数,对数似然损失函数.</li>
<li>损失函数的期望是理论上模型关于联合分布P(X,Y)的平均意义下的损失,称为风险函数,也叫<strong>期望风险</strong>.但是联合分布是未知的,期望风险不能直接计算.</li>
<li>当样本容量N趋于无穷时经验风险趋于期望风险,但现实中训练样本数目有限.</li>
</ol>
<ul>
<li><strong>经验风险最小化和结构风险最小化:</strong></li>
</ul>
<ol>
<li><p>模型关于训练数据集的平均损失称为经验风险.经验风险最小化的策略就是最小化经验风险.当样本数量足够大时学习效果较好.比如当模型是条件概率分布,损失函数是对数损失函数时,经验风险最小化就等价于极大似然估计.但是当样本容量很小时会出现过拟合.  </p>
</li>
<li><p>结构风险最小化等于正则化.结构风险在经验风险上加上表示模型复杂度的正则化项.比如当模型是条件概率分布,损失函数是对数损失函数,模型复杂度由模型的先验概率表示时,结构风险最小化就等价于最大后验概率估计.(一般是在经验风险最小化后面加上正则化项)</p>
</li>
</ol>
<ul>
<li><p><strong>过拟合</strong>是指学习时选择的模型所包含的参数过多,以致于对已知数据预测得很好,但对未知数据预测很差的现象.模型选择旨在避免过拟合并提高模型的预测能力.</p>
</li>
<li><p><strong>正则化</strong>是模型选择的典型方法.正则化项一般是模型复杂度的单调递增函数,比如模型参数向量的范数.</p>
</li>
<li><p><strong>交叉验证</strong>是另一常用的模型选择方法,可分为简单交叉验证,K折交叉验证,留一交叉验证等.</p>
</li>
<li><p><strong>KKT条件</strong>:通常我们要求解的最优化条件有如下三种:</p>
</li>
</ul>
<ol>
<li><p>无约束优化问题:通常使用求导,使导数为零,求解候选最优值</p>
</li>
<li><p>有等式约束的优化问题:通常使用拉格朗日乘子法,即把等式约束用拉格朗日乘子和优化问题合并为一个式子,通过对各个变量求导使其为零,求解候选最优值.拉格朗日乘数法其实是KKT条件在等式约束优化问题的简化版.</p>
</li>
<li><p>有不等式约束的优化问题:通常使用KKT条件.即把不等式约束,等式约束和优化问题合并为一个式子.假设有多个等式约束h(x)和不等式约束g(x),则不等式约束引入的KKT条件如下:,实质是最优解在g(x)&lt;0区域内时,约束条件不起作用,等价于对μ置零然后对原函数的偏导数置零;当g(x)&#x3D;0时与情况2相近.结合两种情况,那么只需要使L对x求导为零,使h(x)为零,使μg(x)为零三式即可求解候选最优值.</p>
</li>
</ol>
<h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><ul>
<li><p>感知机是<strong>二类分类</strong>的线性模型,属于判别模型.感知机学习旨在求出将训练数据进行线性划分的分离超平面.是神经网络和支持向量机的基础.</p>
</li>
<li><p><strong>模型</strong>:<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180322204653743-312134163.png" alt="img">, w叫作权值向量,b叫做偏置,sign是符号函数.</p>
</li>
<li><p><strong>感知机的几何解释</strong>:wx+b对应于特征空间中的一个分离超平面S,其中w是S的法向量,b是S的截距.S将特征空间划分为两个部分,位于两个部分的点分别被分为正负两类.</p>
</li>
<li><p><strong>策略</strong>:假设训练数据集是线性可分的,感知机的损失函数是误分类点到超平面S的总距离.因为误分类点到超平面S的距离是<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180322205619134-1846312193.png" alt="img">,且对于误分类的数据来说,总有<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180322205653727-1828047745.png" alt="img">成立,因此不考虑1&#x2F;||w||,就得到感知机的损失函数:<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180322205749296-770636864.png" alt="img">,其中M是误分类点的集合.感知机学习的<strong>策略就是选取使损失函数最小的模型参数.</strong></p>
</li>
<li><p><strong>算法</strong>:感知机的最优化方法采用<strong>随机梯度下降法</strong>.首先任意选取一个超平面w0,b0,然后不断地极小化目标函数.在极小化过程中一次随机选取一个误分类点更新w,b,直到损失函数为0.<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180322212310170-609164056.png" alt="img">,其中η表示步长.该算法的直观解释是:当一个点被误分类,就调整w,b使分离超平面向该误分类点接近.感知机的解可以不同.</p>
</li>
<li><p><strong>对偶形式</strong>:假设原始形式中的w0和b0均为0,设逐步修改w和b共n次,令a&#x3D;nη,最后学习到的w,b可以表示为<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180322212518323-1783065425.png" alt="img">.那么对偶算法就变为设初始a和b均为0,每次选取数据更新a和b直至没有误分类点为止.对偶形式的意义在于可以将训练集中实例间的内积计算出来,存在Gram矩阵中,可以大大加快训练速度.</p>
</li>
</ul>
<p>感知机学习算法是基于随机梯度下降法的对损失函数的最优化算法，原始形式中，首先任意选取一个超平面，然后用梯度下降法不断极小化目标函数，在这个过程中一次随机选取一个误分类点使其梯度下降</p>
<h3 id="k近邻法"><a href="#k近邻法" class="headerlink" title="k近邻法"></a>k近邻法</h3><ul>
<li>k近邻法根据其<strong>k个最近邻</strong>的训练实例的类别,通过<strong>多数表决</strong>等方式进行预测.k值的选择,距离度量及分类决策规则是k近邻法的三个基本要素.当k&#x3D;1时称为最近邻算法.</li>
<li><strong>模型</strong>:当训练集,距离度量,k值以及分类决策规则确定后,特征空间已经根据这些要素被划分为一些子空间,且子空间里每个点所属的类也已被确定.</li>
<li><strong>策略</strong>:</li>
</ul>
<ol>
<li><strong>距离</strong>:特征空间中两个实例点的距离是相似程度的反映,k近邻算法一般使用欧氏距离(L2范数),也可以使用更一般的Lp距离或Minkowski距离.</li>
<li><strong>k值</strong>:k值较小时,整体模型变得复杂,容易发生过拟合.k值较大时,整体模型变得简单.在应用中k一般取较小的值,通过交叉验证法选取最优的k.</li>
<li><strong>分类决策规则</strong>:k近邻中的分类决策规则往往是多数表决,多数表决规则等价于经验风险最小化.</li>
</ol>
<ul>
<li><strong>算法</strong>:根据给定的距离度量,在训练集中找出与x最邻近的k个点,根据分类规则决定x的类别y.</li>
<li><strong>kd树</strong>:</li>
</ul>
<ol>
<li>kd树就是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构.kd树更适用于<strong>训练实例数远大于空间维数</strong>时的k近邻搜索.(类似二叉搜索树)</li>
<li><strong>构造</strong>:可以通过如下<strong>递归</strong>实现:在超矩形区域上选择一个<strong>坐标轴</strong>和此坐标轴上的一个<strong>切分点</strong>,确定一个超平面,该超平面将当前超矩形区域切分为两个子区域.在子区域上重复切分直到子区域内没有实例时终止.通常依次选择坐标轴和选定坐标轴上的<strong>中位数点</strong>为切分点,这样可以得到平衡kd树.<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180326172916437-1394658265.png" alt="img"></li>
<li><strong>搜索</strong>:从根节点出发,若目标点x当前维的坐标小于切分点的坐标则移动到左子结点,否则移动到右子结点,直到子结点为叶结点为止.以此叶结点为”当前最近点”,<strong>递归</strong>地向上回退,在每个结点:(a)如果该结点比当前最近点距离目标点更近,则以该结点为”当前最近点”(b)”当前最近点”一定存在于该结点一个子结点对应的区域,检查该结点的另一子结点对应的区域是否与以目标点为球心,以目标点与”当前最近点”间的距离为半径的超球体相交.如果相交,移动到另一个子结点,如果不相交,向上回退.持续这个过程直到回退到根结点,最后的”当前最近点”即为最近邻点.<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180326180119864-1117969798.png" alt="img"><img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180326184339257-2103661156.png" alt="img" style="zoom:50%;"></li>
</ol>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><ul>
<li>朴素贝叶斯是基于<strong>贝叶斯定理</strong>和<strong>特征条件独立假设</strong>的分类方法.首先学习输入&#x2F;输出的联合概率分布,然后基于此模型,对给定的输入x,利用贝叶斯定理求出后验概率最大的输出y.属于<strong>生成模型.</strong>(个人认为有点太偏数学计算)</li>
<li><strong>模型</strong>:首先学习先验概率分布<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180326185213330-2005466052.png" alt="img">,然后学习条件概率分布<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180326185741644-1225790408.png" alt="img">.如果估计实际,需要指数级的计算,所以朴素贝叶斯法对条件概率分布作了条件独立性的假设,上式变成<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180326185808936-750113509.png" alt="img">.在分类时,通过学习到的模型计算后验概率分布,由贝叶斯定理得到<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180326185503168-1519818747.png" alt="img">,将条件独立性假设得到的等式代入,并且注意到分母都是相同的,所以得到朴素贝叶斯分类器:<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180326185601591-733611332.png" alt="img"></li>
<li>朴素贝叶斯将实例分到后验概率最大的类中,这等价于期望风险最小化.</li>
<li><strong>算法</strong>:使用<strong>极大似然估计法</strong>估计相应的先验概率<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180326214152861-1013627889.png" alt="img">和条件概率<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180326214207245-476906665.png" alt="img">,计算条件独立性假设下的实例各个取值的可能性<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180326214237922-924326827.png" alt="img">,选取其中的最大值作为输出.</li>
<li>用极大似然估计可能会出现所要估计的概率值为0的情况,在累乘后会影响后验概率的计算结果,使分类产生偏差.可以采用<strong>贝叶斯估计</strong>,在随机变量各个取值的频数上赋予一个正数.<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180326214949942-1671493044.png" alt="img">.Sj为j属性可能取值数量,当λ&#x3D;0时就是极大似然估计.常取λ&#x3D;1,称为<strong>拉普拉斯平滑</strong>.</li>
<li>如果是连续值的情况,可以假设连续变量服从高斯分布,然后用训练数据估计参数.<img src="https://images2018.cnblogs.com/blog/1340182/201804/1340182-20180403122817011-1480997529.png" alt="img"></li>
</ul>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><ul>
<li>决策树是一种基本的分类与回归方法.它可以认为是<strong>if-then规则</strong>的集合,也可以认为是定义在特征空间与类空间上的<strong>条件概率分布</strong>.主要优点是模型具有可读性,分类速度快.</li>
<li><strong>模型</strong>:分类决策树由<strong>结点</strong>和<strong>有向边</strong>组成.结点分为<strong>内部结点</strong>(表示一个特征或属性)和<strong>叶结点</strong>(表示一个类).决策树的路径具有<strong>互斥且完备</strong>的性质.</li>
<li><strong>策略</strong>:决策树学习本质上是从训练数据集中归纳出一组分类规则.我们需要的是一个与训练数据<strong>矛盾较小</strong>,同时具有很好的<strong>泛化能力</strong>的决策树.从所有可能的决策树中选取最优决策树是NP完全问题,所以现实中常采用<strong>启发式方法</strong>近似求解.</li>
<li><strong>算法</strong>:决策树学习算法包含<strong>特征选择</strong>,<strong>决策树的生成</strong>与<strong>决策树的剪枝过程</strong>.生成只考虑局部最优,剪枝则考虑全局最优.</li>
<li><strong>特征选择</strong>:如果利用一个特征进行分类的结果与随机分类的结果没有很大差别,则称这个特征是<strong>没有分类能力</strong>的.扔掉这样的特征对决策树学习的精度影响不大.</li>
</ul>
<ol>
<li><strong>信息熵:<strong>熵是衡量</strong>随机变量不确定性</strong>的度量.熵越大,随机变量的不确定性就越大.信息熵是信息量的期望<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180321003327277-616086724.png" alt="img">.条件熵表示在已知随机变量X的条件下随机变量Y的不确定性.<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327001627042-1130428752.png" alt="img"></li>
<li><strong>信息增益</strong>:表示得知特征X的信息而使得类Y的信息的<strong>不确定性减少</strong>的程度.定义为集合D的经验熵与特征A在给定条件下D的经验条件熵之差<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327001707990-1938505692.png" alt="img">,也就是训练数据集中类与特征的<strong>互信息</strong>.</li>
<li><strong>信息增益算法</strong>:计算数据集D的经验熵<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327002355841-2001061458.png" alt="img">,计算特征A对数据集D的经验条件熵<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327002422268-434662886.png" alt="img">,计算信息增益,选取信息增益最大的特征.</li>
<li><strong>信息增益比</strong>:信息增益值的大小是相对于训练数据集而言的,并无绝对意义.使用信息增益比<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327002749515-35724572.png" alt="img">可以对这一问题进行校正.</li>
</ol>
<ul>
<li><strong>决策树的生成</strong>:</li>
</ul>
<ol>
<li><strong>ID3算法</strong>:核心是在决策树各个结点上应用<strong>信息增益准则</strong>选择信息增益最大且大于阈值的特征,递归地构建决策树.ID3相当于用极大似然法进行概率模型的选择.由于算法只有树的生成,所以容易产生过拟合.</li>
<li><strong>C4.5算法</strong>:C4.5算法与ID3算法相似,改用<strong>信息增益比</strong>来选择特征.</li>
</ol>
<ul>
<li><strong>决策树的剪枝</strong>:</li>
</ul>
<ol>
<li>在学习时过多考虑如何提高对训练数据的正确分类,从而构建出过于复杂的决策树,产生<strong>过拟合</strong>现象.解决方法是对已生成的决策树进行简化,称为剪枝.</li>
<li>设树的叶结点个数为|T|,每个叶结点有Nt个样本点,其中k类样本点有Ntk个,剪枝往往通过极小化决策树整体的损失函数<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327184108829-491568588.png" alt="img">来实现,其中经验熵<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327184126492-1318761626.png" alt="img">.剪枝通过加入a|T|项来考虑模型复杂度,实际上就是用正则化的极大似然估计进行模型选择.</li>
<li><strong>剪枝算法</strong>:剪去某一子结点,如果生成的新的整体树的<strong>损失函数值</strong>小于原树,则进行剪枝,直到不能继续为止.具体可以由动态规划实现.</li>
</ol>
<ul>
<li><strong>CART算法</strong>:</li>
</ul>
<ol>
<li>CART既可以用于<strong>分类也</strong>可以用于<strong>回归</strong>.它假设决策树是<strong>二叉树</strong>,内部结点特征的取值为”是”和”否”.递归地构建二叉树,对回归树用<strong>平方误差</strong>最小化准则,对分类数用<strong>基尼指数</strong>最小化准则.</li>
<li><strong>回归树的生成</strong>:在训练数据集所在的输入空间中,递归地将每个区域划分为两个子区域.选择第j个变量和它取的值s作为切分变量和切分点,并定义两个区域<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327192458368-454922185.png" alt="img">,遍历变量j,对固定的j扫描切分点s,求解<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327192515798-625636184.png" alt="img">.用选定的对(j,s)划分区域并决定相应的输出值<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327192639203-2128485765.png" alt="img">,直到满足停止条件.</li>
<li><strong>基尼指数</strong>:假设有K个类,样本属于第k类的概率为pk,则概率分布的基尼指数为<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327232213959-1850846533.png" alt="img">,表示不确定性.在特征A的条件下集合D的基尼指数定义为<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327232413235-202559527.png" alt="img">,表示分割后集合D的不确定性.基尼指数越大,样本集合的<strong>不确定性</strong>也就越大.     <em><strong>*基尼指数是信息熵中﹣logP在P&#x3D;1处一阶泰勒展开后的结果*</strong></em></li>
<li><strong>分类树的生成</strong>:从根结点开始,递归进行以下操作:设结点的训练数据集为D,对每个特征A和其可能取的每个值a,计算A&#x3D;a时的基尼指数,选择<strong>基尼指数最小</strong>的特征及其对应的切分点作为<strong>最优特征</strong>与<strong>最优切分点</strong>,生成两个子结点,直至满足<strong>停止条件</strong>.停止条件一般是结点中的样本个数小于阈值,或样本集的基尼指数小于阈值,或没有更多特征.</li>
<li><strong>CART剪枝</strong>:<br>Tt表示以t为根结点的子树,|Tt|是Tt的叶结点个数.可以证明当<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327234132329-345717665.png" alt="img">时,Tt与t有相同的损失函数值,且t的结点少,因此t比Tt更可取,对Tt进行剪枝.<strong>自下而上</strong>地对各内部结点t计算<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327233730043-1128896018.png" alt="img">,并令a&#x3D;min(g(t)),<strong>自上而下</strong>地访问内部节点t,如果有g(t)&#x3D;a,进行剪枝,并对t以<strong>多数表决法</strong>决定其类,得到子树T,如此循环地生成一串<strong>子树序列</strong>,直到新生成的T是由根结点单独构成的树为止.利用<strong>交叉验证法</strong>在子树序列中选取最优子树.</li>
</ol>
<ul>
<li>如果是<strong>连续值</strong>的情况,一般用<strong>二分法</strong>作为结点来划分.</li>
</ul>
<h3 id="logistic回归和最大熵模型"><a href="#logistic回归和最大熵模型" class="headerlink" title="logistic回归和最大熵模型"></a>logistic回归和最大熵模型</h3><ul>
<li><strong>逻辑斯谛分布</strong>:<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327235539236-2120997612.png" alt="img">分布函数f(x)以点(μ,1&#x2F;2)为中心对称,γ的值越小,曲线在中心附近增长得越快.</li>
<li><strong>逻辑斯谛回归模型</strong>:对于给定的输入x,根据<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327235903221-600532158.png" alt="img">和<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180327235921507-1062845571.png" alt="img">计算出两个条件概率值的大小,将x分到概率值较大的那一类.将偏置b加入到权值向量w中,并在x的最后添加常数项1,得到<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328000053180-338790070.png" alt="img">和<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328000100672-1305811749.png" alt="img">.如果某事件发生的概率是p,则该事件发生的<strong>几率</strong>(此处几率指该事件发生概率与不发生概率之比)是p&#x2F;1-p,<strong>对数几率</strong>是log(p&#x2F;1-p),那么<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328000401520-441393571.png" alt="img">,也就是说在逻辑斯谛回归模型中,输出Y&#x3D;1的对数几率是输入x的<strong>线性函数</strong>,线性函数值越接近正无穷,概率值就越接近1,反之则越接近0.</li>
<li><strong>似然估计</strong>:给定x的情况下参数θ是真实参数的可能性.</li>
<li><strong>模型参数估计</strong>:对于给定的二分类训练数据集,对数似然函数为<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328001235739-273773705.png" alt="img">,也就是<strong>损失函数</strong>.其中P(Y&#x3D;1|x)&#x3D;π(x),对L(w)求极大值,就可以得到w的估计值.问题变成了以对数似然函数为目标函数的最优化问题.</li>
<li><strong>多项逻辑斯谛回归</strong>: 当问题是多分类问题时,可以作如下推广:设Y有K类可能取值,<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328152026773-995021041.png" alt="img">,<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328152039974-1638092291.png" alt="img">,实际上就是<strong>one-vs-all</strong>的思想,将其他所有类当作一个类,问题转换为二分类问题.</li>
<li><strong>最大熵原理</strong>:学习概率模型时,在所有可能的概率模型中,<strong>熵最大</strong>的模型是最好的模型.直观地,最大熵原理认为模型首先要满足已有的事实,即<strong>约束条件</strong>.在没有更多信息的情况下,那些不确定的部分都是”<strong>等可能的</strong>“.</li>
<li><strong>最大熵模型</strong>:给定训练数据集,可以确定联合分布P(X,Y)的经验分布<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328154127295-1500038043.png" alt="img">和边缘分布P(X)的经验分布<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328154141776-1634765352.png" alt="img">,其中v表示频数,N表示样本容量.用<strong>特征函数f(x,y)</strong>&#x3D;1描述x与y满足某一事实,可以得到特征函数关于P(X,Y)的经验分布的期望值和关于模型P(Y|X)与P(X)的经验分布的期望值,假设两者相等,就得到了<strong>约束条件</strong><img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328154435423-22674055.png" alt="img">.定义在条件概率分布P(Y|X)上的条件熵为<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328154526636-172871135.png" alt="img">,则<strong>条件熵最大</strong>的模型称为最大熵模型.</li>
<li><strong>最大熵模型的学习</strong>就是求解最大熵模型的过程.等价于<strong>约束最优化问题</strong><img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328154947665-666670231.png" alt="img">,将求最大值问题改为等价的求最小值问题<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328155017756-728051002.png" alt="img">.引入<strong>拉格朗日乘子</strong><img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328161703775-262697225.png" alt="img">将原始问题<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328155306376-816726174.png" alt="img">转换为无约束最优化的<strong>对偶问题</strong><img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328155321425-378767091.png" alt="img">.首先求解内部的<strong>极小化问题</strong>,即求L(P,W)对P(y|x)的偏导数<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328161842486-1531031915.png" alt="img">,并令偏导数等于0,解得<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328160359980-791888704.png" alt="img">.可以证明对偶函数等价于对数似然函数,那么对偶函数极大化等价于最大熵模型的<strong>极大似然估计<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180328164249608-1941769273.png" alt="img"></strong>.之后可以用最优化算法求解得到w.</li>
<li>最大熵模型与逻辑斯谛回归模型有类似的形式,它们又称为<strong>对数线性模型</strong>.模型学习就是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计.</li>
<li><strong>算法</strong>:似然函数是<strong>光滑的凸函数</strong>,因此多种最优化方法都适用.</li>
</ul>
<ol>
<li><strong>改进的迭代尺度法(IIS)</strong>:假设当前的参数向量是w,如果能找到一种方法<strong>w-&gt;w+δ</strong>使对数似然函数值变大,就可以<strong>重复</strong>使用这一方法,直到找到最大值.</li>
<li>逻辑斯谛回归常应用梯度下降法,牛顿法或拟牛顿法.</li>
</ol>
<p>​     <strong>两个都是模型，类似Relu和sigmoid函数</strong></p>
<h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h3><ul>
<li><strong>模型</strong>:支持向量机(SVM)是一种<strong>二类分类模型</strong>.它的基本模型是定义在特征空间上的<strong>间隔最大</strong>的线性分类器.支持向量机还包括<strong>核技巧</strong>,使它成为实质上的非线性分类器.<strong>分离超平面</strong><img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180330002239570-127918255.png" alt="img">,<strong>分类决策函数</strong><img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180330002251982-1718192791.png" alt="img">.</li>
<li><strong>策略</strong>:<strong>间隔最大化</strong>,可形式化为一个求解<strong>凸二次规划</strong>的问题,也等价于正则化的<strong>合页损失函数</strong>的最小化问题. (<strong>二次规划：在限制田间Ax&lt;&#x3D;b的条件下，找一个n维的向量x，使得fx&#x3D;1&#x2F;2 X转置QX+C转置X为最小</strong> <strong>凸二次规划：凸二次函数的二次规划</strong>)</li>
<li>当训练数据<strong>线性可分</strong>时,通过硬间隔最大化,学习出<strong>线性可分支持向量机</strong>.当训练数据<strong>近似线性可分</strong>时,通过软间隔最大化,学习出<strong>线性支持向量机</strong>.当训练数据<strong>线性不可分</strong>时,通过使用核技巧及软间隔最大化,学习<strong>非线性支持向量机</strong>.</li>
<li><strong>核技巧</strong>:当输入空间为欧式空间或离散集合,特征空间为希尔伯特空间时,核函数表示将输入从输入空间<strong>映射</strong>到特征空间得到的特征向量之间的<strong>内积</strong>.通过核函数学习非线性支持向量机等价于在高维的特征空间中学习线性支持向量机.这样的方法称为核技巧.</li>
<li>考虑一个二类分类问题,假设输入空间与特征空间为两个不同的空间,输入空间为<strong>欧氏空间或离散集合</strong>,特征空间为<strong>欧氏空间或希尔伯特空间</strong>.支持向量机都将输入映射为特征向量,所以支持向量机的学习是在<strong>特征空间</strong>进行的.</li>
<li>支持向量机的最优化问题一般通过对偶问题化为<strong>凸二次规划问题</strong>求解,具体步骤是将等式约束条件代入优化目标,通过求偏导求得优化目标在不等式约束条件下的极值.</li>
<li><strong>线性可分支持向量机</strong>:</li>
</ul>
<ol>
<li><p>当训练数据集线性可分时,存在无穷个分离超平面可将两类数据正确分开.利用<strong>间隔最大化</strong>得到<strong>唯一</strong>最优分离超平面<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329195644514-423540564.png" alt="img">和相应的分类决策函数<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329195710679-1196197596.png" alt="img">称为线性可分支持向量机.</p>
</li>
<li><p><strong>函数间隔</strong>:一般来说,一个点距离分离超平面的<strong>远近</strong>可以表示分类预测的<strong>确信程度</strong>.在超平面<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329193801832-1872817034.png" alt="img">确定的情况下,|wx+b|能够相对地表示点x距离超平面的远近,而wx+b与y的符号是否一致能够表示分类是否正确.所以可用<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329193924366-1442320220.png" alt="img">来表示分类的正确性及确信度,这就是<strong>函数间隔</strong>.注意到即使超平面不变,函数间隔仍会受w和b的绝对大小影响.</p>
</li>
<li><p><strong>几何间隔</strong>:一般地,当样本点被超平面正确分类时,点x与超平面的距离是<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329194312732-1659585878.png" alt="img">,其中||w||是w的<strong>l2范数</strong>.这就是<strong>几何间隔</strong>的定义.定义超平面关于训练数据集T的几何间隔为超平面关于T中所有样本点的几何间隔之<strong>最小值</strong><img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329194441854-924170360.png" alt="img">.可知<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329194919109-451943286.png" alt="img">,当||w||&#x3D;1时几何间隔和函数间隔<strong>相等</strong>.</p>
</li>
<li><p><strong>硬间隔最大化</strong>:对线性可分的训练集而言,这里的间隔最大化又称为<strong>硬间隔最大化</strong>.直观解释是对训练集找到几何间隔最大的超平面意味着以<strong>充分大的确信度</strong>对训练数据进行分类.求最大间隔分离超平面即约束最优化问题:<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329195056137-797393789.png" alt="img"></p>
</li>
<li><p>将几何间隔用函数间隔表示<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329195259915-1554416502.png" alt="img">,并且注意到函数间隔的取值并不影响最优化问题的解,不妨令函数间隔&#x3D;1,并让最大化1&#x2F;||w||等价为最小化||w||^2&#x2F;2,问题变为<strong>凸二次规划问题</strong><img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329195439002-746730218.png" alt="img">.</p>
</li>
<li><p><strong>支持向量和间隔边界</strong>:与分离超平面距离<strong>最近的样本点</strong>的实例称为<strong>支持向量</strong>.支持向量是使最优化问题中的约束条件等号成立的点.因此对y&#x3D;+1的正例点和y&#x3D;-1的负例点,支持向量分别在超平面H1:wx+b&#x3D;+1和H2:wx+b&#x3D;-1.H1和H2平行,两者之间形成一条长带,长带的宽度<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329200510820-1049834507.png" alt="img">称为<strong>间隔</strong>,H1和H2称为<strong>间隔边界</strong>.在决定分离超平面时只有支持向量起作用,所以支持向量机是由很少的”重要的”训练样本确定的.由对偶问题同样可以得到支持向量一定在间隔边界上.<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329200803273-756823451.png" alt="img"></p>
</li>
<li><p><strong>对偶算法</strong>: 引进拉格朗日乘子,定义拉格朗日函数<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329210528603-1215606072.png" alt="img">,根据拉格朗日对偶性,原始问题的对偶问题是极大极小问题:<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329210613559-1081388066.png" alt="img">.先求对w,b的<strong>极小值</strong>.将L(w,b,a)分别对w,b求偏导数并令其等于0,得<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329210839961-390121294.png" alt="img">,代入拉格朗日函数得</p>
<p><img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329211101130-686745660.png" alt="img">,这就是极小值.接下来对极小值求对a的极大,即是<strong>对偶问题</strong><img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329211520286-2109508118.png" alt="img">.</p>
<p>将求极大转换为求极小<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329211543019-900362367.png" alt="img">.由<strong>KKT条件</strong>成立得到<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180329211825920-547805707.png" alt="img">,其中j为使aj*&gt;0的下标之一.所以问题就变为求对偶问题的解a*,再求得原始问题的解w*,b*,从而得分离超平面及分类决策函数可以看出w<em>和b</em>都只依赖训练数据中ai*&gt;0的样本点(xi,yi),这些实例点xi被称为<strong>支持向量</strong>.</p>
</li>
</ol>
<ul>
<li><strong>线性支持向量机</strong>:</li>
</ul>
<ol>
<li>如果训练数据是<strong>线性不可分</strong>的,那么上述方法中的不等式约束并不能都成立,需要修改硬间隔最大化,使其成为<strong>软间隔最大化</strong>.</li>
<li>线性不可分意味着某些<strong>特异点</strong>不能满足函数间隔大于等于1的约束条件,可以对每个样本点引进一个<strong>松弛变量</strong>,使函数间隔加上松弛变量大于等于1,约束条件变为<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180330000759666-1236333718.png" alt="img">,同时对每个松弛变量,支付一个代价,目标函数变为<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180330000826533-10368132.png" alt="img">,其中C&gt;0称为<strong>惩罚参数</strong>,C值越大对误分类的惩罚也越大.新目标函数包含了两层含义:使<strong>间隔尽量大</strong>,同时使误分类点的<strong>个数尽量小</strong>.</li>
<li><strong>软间隔最大化</strong>:学习问题变成如下<strong>凸二次规划</strong>问题:<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180330002102314-733785177.png" alt="img"></li>
<li>可以证明w的解是唯一的,但b的解存在一个<strong>区间</strong>.线性支持向量机包含线性可分支持向量机,因此<strong>适用性更广</strong>.</li>
<li><strong>对偶算法</strong>: 原始问题的对偶问题是,构造<strong>拉格朗日函数</strong><img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180330002603621-1915919714.png" alt="img">,先求对w,b,ξ的<strong>极小值</strong>,分别求偏导并令导数为0,得<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180330002933581-2090154018.png" alt="img">,代入原函数,再对极小值求a的<strong>极大值</strong>,得到<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180330003059767-1409058556.png" alt="img">,</li>
<li>利用后三条约束<strong>消去μ</strong>,再将求极大转换为<strong>求极小</strong>,得到<strong>对偶问题</strong><img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180330003236977-856067561.png" alt="img">.</li>
<li>由<strong>KKT条件</strong>成立可以得到<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180330003423558-198281251.png" alt="img">,j是满足0&lt;aj*&lt;C的下标之一.问题就变为选择惩罚参数C&gt;0,求得对偶问题(<strong>凸二次规划问题</strong>)的<strong>最优解a*</strong>,代入计算w<em>和b</em>,求得分离超平面和分类决策函数.因为b的解并不唯一,所以实际计算b<em>时可以取所有样本点上的*<em>平均值</em></em>.</li>
<li><strong>支持向量</strong>:在<strong>线性不可分</strong>的情况下,将对应与ai*&gt;0的样本点(xi,yi)的实例点xi称为<strong>支持向量</strong>.软间隔的支持向量或者在间隔边界上,或者在间隔边界与分类超平面之间,或者再分离超平面误分一侧.<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180330004857328-39247694.png" alt="img"></li>
<li><strong>合页损失函数</strong>:可以认为是0-1损失函数的上界,而线性支持向量机可以认为是优化合页损失函数构成的目标函数.<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180330005743355-1368753376.png" alt="img"></li>
</ol>
<ul>
<li><strong>非线性支持向量机</strong>:</li>
</ul>
<ol>
<li>如果分类问题是<strong>非线性</strong>的,就要使用<strong>非线性支持向量机</strong>.主要特点是使用<strong>核技巧</strong>.</li>
<li><strong>非线性分类问题</strong>:用线性分类方法求解非线性分类问题分为两步:首先使用一个变换将原空间的数据映射到新空间,然后在新空间里用线性分类学习方法从训练数据中学习分类模型.</li>
<li><strong>核函数</strong>:设X是<strong>输入空间</strong>(欧式空间的子集或离散集合),H为<strong>特征空间</strong>(希尔伯特空间),一般是<strong>高维</strong>甚至无穷维的.如果存在一个从X到H的映射<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180330220915181-1089999737.png" alt="img">使得对所有x,z属于X,函数K(x,z)满足条件<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180330220934830-1744701619.png" alt="img">,点乘代表<strong>内积</strong>,则称K(x,z)为<strong>核函数</strong>.</li>
<li><strong>核技巧</strong>:基本思想是通过一个<strong>非线性变换</strong>将输入空间对应于一个<strong>特征空间</strong>,使得在输入空间中的<strong>超曲面模型</strong>对应于特征空间中的<strong>超平面模型</strong>(支持向量机).在学习和预测中只定义核函数K(x,z),而<strong>不显式</strong>地定义映射函数.对于给定的核K(x,z),特征空间和映射函数的取法并<strong>不唯一</strong>.注意到在线性支持向量机的对偶问题中,目标函数和决策函数都只涉及输入实例与实例之间的<strong>内积</strong>,xi<code>xj可以用核函数K(xi,xj)=Ф(xi)</code>Ф(xj)来<strong>代替</strong>.当映射函数是非线性函数时,学习到的含有核函数的支持向量机是非线性分类模型.在实际应用中,往往依赖领域知识<strong>直接选择</strong>核函数.</li>
<li><strong>正定核</strong>:通常所说的核函数是指<strong>正定核函数</strong>.只要满足正定核的充要条件,那么给定的函数K(x,z)就是正定核函数.设K是定义在X<em>X上的*<em>对称函数</em></em>,如果任意xi属于X,K(x,z)对应的<strong>Gram矩阵</strong><img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180330231940071-183800349.png" alt="img">是<strong>半正定矩阵</strong>,则称K(x,z)是正定核.这一定义在构造核函数时很有用,但要验证一个具体函数是否为正定核函数并不容易,所以在实际问题中往往应用已有的核函数.</li>
<li><strong>算法</strong>:选取适当的核函数K(x,z)和适当的参数C,将线性支持向量机对偶形式中的内积换成核函数,构造并求解最优化问题<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180331124957775-985382044.png" alt="img">,选择最优解a<em>的一个正分量0&lt;aj</em>&lt;C计算<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180331125030345-1281269762.png" alt="img">,构造决策函数<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180331125050863-2089383445.png" alt="img">.</li>
</ol>
<ul>
<li><strong>常用核函数</strong>:</li>
</ul>
<ol>
<li><strong>多项式核函数(polynomial kernel function)</strong>:<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180331020352594-820833872.png" alt="img">,对应的支持向量机是一个p次多项式分类器,分类决策函数为<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180331020424696-1187788653.png" alt="img">.</li>
<li><strong>高斯核函数(Gaussian krenel function)</strong>:<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180331020542789-1476679312.png" alt="img">,对应的支持向量机是高斯径向基函数(RBF)分类器.分类决策函数为<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180331020620823-461043075.png" alt="img">.</li>
<li><strong>字符串核函数(string kernel function)</strong>: 核函数不仅可以定义在欧氏空间上,还可以定义在<strong>离散数据的集合</strong>上.字符串核函数给出了字符串中长度等于n的所有子串组成的特征向量的余弦相似度.</li>
</ol>
<ul>
<li><strong>序列最小最优化(SMO)算法</strong>:</li>
</ul>
<ol>
<li>SMO是一种<strong>**快速求解凸二次规划问题<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180331130317813-99877192.png" alt="img">*<em><strong>的算法.基本思路是:如果所有变量都满足此优化问题的KKT条件,那么解就得到了.否则,选择</strong>两个变量</em>*,固定其他变量,针对这两个变量构建一个二次规划问题.不断地将原问题分解为</strong>子问题<strong>并对子问题求解,就可以求解原问题.注意子问题两个变量中只有一个是</strong>自由变量**,另一个由<strong>等式约束</strong>确定.</li>
<li><strong>两个变量二次规划的求解方法</strong>:假设选择的两个变量是a1,a2,其他变量是固定的,于是得到子问题<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180331130636006-16365695.png" alt="img">,ε是常数,目标函数式省略了不含a1,a2的常数项.考虑不等式约束和等式约束,要求的是目标函数在一条平行于对角线的线段上的最优值<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180331130951922-273284856.png" alt="img">,问题变为<strong>单变量</strong>的最优化问题.假设初始可行解为aold,最优解为anew,考虑沿着约束方向未经剪辑的最优解anew,unc(即未考虑不等式约束).对该问题求偏导数,并令导数为0,代入原式,令<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180331174156407-1692428298.png" alt="img">,得到<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180331173504675-776282093.png" alt="img">,经剪辑后a2的解是<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180331173757102-1465555454.png" alt="img">,L与H是a2new所在的对角线段端点的界.并解得<img src="https://images2018.cnblogs.com/blog/1340182/201803/1340182-20180331173517302-1011653073.png" alt="img">.</li>
<li><strong>变量的选择方法</strong>:在每个子问题中选择两个变量优化,其中至少一个变量是违反KKT条件的.第一个变量的选取标准是<strong>违反KKT条件最严重</strong>的样本点,第二个变量的选取标准是希望能使该变量有<strong>足够大的变化</strong>,一般可以选取使对应的|E1-E2|最大的点.在每次选取完点后,<strong>更新</strong>阈值b和差值Ei.</li>
</ol>
<p>经验：1.N维可分，则&gt;N维时必可分(数据集必定会在某个维度变得可分)</p>
<p>​            2.几何间隔最大的分离超平面存在且唯一</p>
<p>​            3.SVM主要思路时求解能够正确划分训练集并且间隔最大的分离超平面。</p>
<p>​            4.数据集线性不可分时，多项式回归(核函数)通过将数据从低维映射到高维，将原本线性不可分 </p>
<p>​               的情况变为可分。</p>
<p>​            </p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://olstussy.top">Olstussy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://olstussy.top/2023/09/10/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1-7%E7%AB%A0)/">https://olstussy.top/2023/09/10/统计学习笔记(1-7章)/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://olstussy.top" target="_blank">Olstussy</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/Numpy/">Numpy</a><a class="post-meta__tags" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/">统计学</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2023/08/02/UOWRIZPuaAX81yL.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/09/10/%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/" title="AI经典论文综述"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">AI经典论文综述</div></div></a></div><div class="next-post pull-right"><a href="/2023/08/20/cs231n(%E4%B8%89)/" title="cs231n(三)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">cs231n(三)</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/08/02/numpy%E7%AC%94%E8%AE%B0/" title="Numpy笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-02</div><div class="title">Numpy笔记</div></div></a></div><div><a href="/2023/09/10/%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/" title="AI经典论文综述"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-10</div><div class="title">AI经典论文综述</div></div></a></div><div><a href="/2023/08/02/2.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" title="AI笔记（2）：神经网络基础"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-02</div><div class="title">AI笔记（2）：神经网络基础</div></div></a></div><div><a href="/2023/08/03/4.%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="AI笔记（4）：深层神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-03</div><div class="title">AI笔记（4）：深层神经网络</div></div></a></div><div><a href="/2023/08/03/3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="AI笔记（3）：浅层神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-03</div><div class="title">AI笔记（3）：浅层神经网络</div></div></a></div><div><a href="/2023/08/03/5.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8/" title="AI笔记（5）：神经网络应用"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-03</div><div class="title">AI笔记（5）：神经网络应用</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2023/08/02/UOWRIZPuaAX81yL.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Olstussy</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/olstussy"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">No Error No Warning !</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">1.</span> <span class="toc-text">感知机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#k%E8%BF%91%E9%82%BB%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">k近邻法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="toc-number">3.</span> <span class="toc-text">朴素贝叶斯</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">4.</span> <span class="toc-text">决策树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#logistic%E5%9B%9E%E5%BD%92%E5%92%8C%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.</span> <span class="toc-text">logistic回归和最大熵模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">6.</span> <span class="toc-text">支持向量机</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/07/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%9F%BA%E7%A1%80/" title="区块链基础">区块链基础</a><time datetime="2023-12-07T08:32:21.000Z" title="发表于 2023-12-07 16:32:21">2023-12-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/11/24/Kripe%20Structure/" title="Kripe结构">Kripe结构</a><time datetime="2023-11-24T08:32:21.000Z" title="发表于 2023-11-24 16:32:21">2023-11-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/11/24/%E6%95%B0%E7%90%86%E9%80%BB%E8%BE%91%E5%9F%BA%E7%A1%80/" title="数理逻辑基础">数理逻辑基础</a><time datetime="2023-11-24T08:32:21.000Z" title="发表于 2023-11-24 16:32:21">2023-11-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/09/10/%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/" title="AI经典论文综述">AI经典论文综述</a><time datetime="2023-09-10T04:40:21.000Z" title="发表于 2023-09-10 12:40:21">2023-09-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/09/10/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1-7%E7%AB%A0)/" title="统计学习笔记(1-7章)">统计学习笔记(1-7章)</a><time datetime="2023-09-10T02:36:21.000Z" title="发表于 2023-09-10 10:36:21">2023-09-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By Olstussy</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>