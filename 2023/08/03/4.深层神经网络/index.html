<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>AI笔记（4）：深层神经网络 | Olstussy</title><meta name="author" content="Olstussy"><meta name="copyright" content="Olstussy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1.深层神经网络我们在前面提到了浅层神经网络，深层神经网络其实就是包含更多隐层的神经网络。下图分别列举了不同深度的神经网络模型结构：  我们会参考「隐层个数」和「输出层」对齐命名。如上图逻辑回归可以叫做1 layer NN，单隐层神经网络可以叫做2 layer NN，2个隐层的神经网络叫做3 layer NN，以此类推。所以当我们提到L layer NN，指的是包含  个隐层的神经网络。 下面">
<meta property="og:type" content="article">
<meta property="og:title" content="AI笔记（4）：深层神经网络">
<meta property="og:url" content="https://olstussy.top/2023/08/03/4.%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="Olstussy">
<meta property="og:description" content="1.深层神经网络我们在前面提到了浅层神经网络，深层神经网络其实就是包含更多隐层的神经网络。下图分别列举了不同深度的神经网络模型结构：  我们会参考「隐层个数」和「输出层」对齐命名。如上图逻辑回归可以叫做1 layer NN，单隐层神经网络可以叫做2 layer NN，2个隐层的神经网络叫做3 layer NN，以此类推。所以当我们提到L layer NN，指的是包含  个隐层的神经网络。 下面">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/08/02/UOWRIZPuaAX81yL.jpg">
<meta property="article:published_time" content="2023-08-03T07:41:07.000Z">
<meta property="article:modified_time" content="2023-08-03T15:50:30.682Z">
<meta property="article:author" content="Olstussy">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/08/02/UOWRIZPuaAX81yL.jpg"><link rel="shortcut icon" href="/img/favicon-32x32-next.png"><link rel="canonical" href="https://olstussy.top/2023/08/03/4.%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'AI笔记（4）：深层神经网络',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-08-03 23:50:30'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2023/08/02/UOWRIZPuaAX81yL.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2023/08/02/n2kWORi8zyCmNXH.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Olstussy"><span class="site-name">Olstussy</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">AI笔记（4）：深层神经网络</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-03T07:41:07.000Z" title="发表于 2023-08-03 15:41:07">2023-08-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-08-03T15:50:30.682Z" title="更新于 2023-08-03 23:50:30">2023-08-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="AI笔记（4）：深层神经网络"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><meta name="referrer" content="no-referrer">

<h2 id="1-深层神经网络"><a href="#1-深层神经网络" class="headerlink" title="1.深层神经网络"></a>1.深层神经网络</h2><p>我们在前面提到了浅层神经网络，深层神经网络其实就是包含更多隐层的神经网络。下图分别列举了不同深度的神经网络模型结构：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/fe1b0d2a870df4aa84d2dbb262eee095.png" alt="深层神经网络"></p>
<p>我们会参考「隐层个数」和「输出层」对齐命名。如上图逻辑回归可以叫做<strong>1 layer NN</strong>，单隐层神经网络可以叫做<strong>2 layer NN</strong>，2个隐层的神经网络叫做<strong>3 layer NN</strong>，以此类推。所以当我们提到<strong>L layer NN</strong>，指的是包含 <img src="https://www.zhihu.com/equation?tex=L-1" alt="公式"> 个隐层的神经网络。</p>
<p>下面我们来了解一下神经网络的一些标记写法。以如下图的<strong>4层神经网络</strong>为例：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/a39a6af8b64d1f10b20b7bec8de24997.png" alt="深层神经网络"></p>
<ol start="11">
<li>总层数用 <img src="https://www.zhihu.com/equation?tex=L" alt="公式"> 表示， <img src="https://www.zhihu.com/equation?tex=L=4" alt="公式"></li>
</ol>
<ul>
<li><p>输入层是第 <img src="https://www.zhihu.com/equation?tex=0" alt="公式"> 层，输出层是第 <img src="https://www.zhihu.com/equation?tex=L" alt="公式"> 层</p>
</li>
<li><p><img src="https://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D" alt="公式"> 表示第 <img src="https://www.zhihu.com/equation?tex=l" alt="公式"> 层包含的单元个数， <img src="https://www.zhihu.com/equation?tex=l=0,1,%5Ccdots,L" alt="公式"></p>
</li>
<li><p>下图模型中， <img src="https://www.zhihu.com/equation?tex=n%5E%7B%5B0%5D%7D=n_x=3" alt="公式">，表示三个输入特征 <img src="https://www.zhihu.com/equation?tex=x_1" alt="公式"> 、<img src="https://www.zhihu.com/equation?tex=x_2" alt="公式"> 、<img src="https://www.zhihu.com/equation?tex=x_3" alt="公式"></p>
</li>
<li><p>下图模型中 <img src="https://www.zhihu.com/equation?tex=n%5E%7B%5B1%5D%7D=5" alt="公式">，<img src="https://www.zhihu.com/equation?tex=n%5E%7B%5B2%5D%7D=5" alt="公式">，<img src="https://www.zhihu.com/equation?tex=n%5E%7B%5B3%5D%7D=3" alt="公式">，<img src="https://www.zhihu.com/equation?tex=n%5E%7B%5B4%5D%7D=n%5E%7B%5BL%5D%7D=1" alt="公式"></p>
</li>
<li><p>第 <img src="https://www.zhihu.com/equation?tex=l" alt="公式"> 层的激活函数输出用 <img src="https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D" alt="公式"> 表示，<img src="https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D=g%5E%7B%5Bl%5D%7D(z%5E%7B%5Bl%5D%7D)" alt="公式"></p>
</li>
<li><p><img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="公式"> 表示第 <img src="https://www.zhihu.com/equation?tex=l" alt="公式"> 层的权重，用于计算 <img src="https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D" alt="公式"></p>
</li>
<li><p>输入 <img src="https://www.zhihu.com/equation?tex=x" alt="公式"> 记为 <img src="https://www.zhihu.com/equation?tex=a%5E%7B%5B0%5D%7D" alt="公式"></p>
</li>
<li><p>输出层 <img src="https://www.zhihu.com/equation?tex=%5Chat%20y" alt="公式"> 记为 <img src="https://www.zhihu.com/equation?tex=a%5E%7B%5BL%5D%7D" alt="公式"></p>
</li>
</ul>
<blockquote>
<p>注意， <img src="https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="公式"> 中的上标 <img src="https://www.zhihu.com/equation?tex=l" alt="公式"> 都是从1开始的，<img src="https://www.zhihu.com/equation?tex=l=1,%5Ccdots,L" alt="公式">。</p>
</blockquote>
<h2 id="2-深层神经网络前向运算"><a href="#2-深层神经网络前向运算" class="headerlink" title="2.深层神经网络前向运算"></a>2.深层神经网络前向运算</h2><p>下面我们来推导一下深层神经网络的前向传播计算过程。依旧是上面提到的4层神经网络，我们以其为例来做讲解。</p>
<h3 id="2-1-单个样本的计算"><a href="#2-1-单个样本的计算" class="headerlink" title="2.1 单个样本的计算"></a>2.1 单个样本的计算</h3><p>对于<strong>单个样本</strong>，我们有：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/aca91359be984aed66ae23b1142f8ad5.png" alt="深层神经网络前向运算"></p>
<h3 id="2-2-m个样本的批量计算"><a href="#2-2-m个样本的批量计算" class="headerlink" title="2.2 m个样本的批量计算"></a>2.2 m个样本的批量计算</h3><p>对于 <img src="https://www.zhihu.com/equation?tex=m" alt="公式"> 个训练样本的情况，我们以<strong>向量化矩阵形式</strong>来并行计算：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/b28cf1ac43f39d83adbe1792eefc9945.png" alt="深层神经网络前向运算"></p>
<p>以此类推，对于第 <img src="https://www.zhihu.com/equation?tex=l" alt="公式"> 层，其前向传播过程的 <img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D" alt="公式"> 可以表示为：</p>
<p><img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D=W%5E%7B%5Bl%5D%7DA%5E%7B%5Bl-1%5D%7D+b%5E%7B%5Bl%5D%7D" alt="公式"></p>
<p><img src="https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D=g%5E%7B%5Bl%5D%7D(Z%5E%7B%5Bl%5D%7D)" alt="公式"></p>
<ul>
<li>其中 <img src="https://www.zhihu.com/equation?tex=l=1,%5Ccdots,L" alt="公式"></li>
</ul>
<h2 id="3-向量化形态下的矩阵维度"><a href="#3-向量化形态下的矩阵维度" class="headerlink" title="3.向量化形态下的矩阵维度"></a>3.向量化形态下的矩阵维度</h2><p>在单个训练样本的场景下，输入 <img src="https://www.zhihu.com/equation?tex=x" alt="公式"> 的维度是 <img src="https://www.zhihu.com/equation?tex=(n%5E%7B%5B0%5D%7D,1)" alt="公式"> 神经网络的参数 <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D" alt="公式"> 的维度分别是：</p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D:%20(n%5E%7B%5Bl%5D%7D,n%5E%7B%5Bl-1%5D%7D)" alt="公式"></li>
<li><img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D:%20(n%5E%7B%5Bl%5D%7D,1)" alt="公式"></li>
</ul>
<p>其中，</p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=l=1,%5Ccdots,L" alt="公式"></li>
<li><img src="https://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=n%5E%7B%5Bl-1%5D%7D" alt="公式"> 分别表示第 <img src="https://www.zhihu.com/equation?tex=l" alt="公式"> 层和 <img src="https://www.zhihu.com/equation?tex=l-1" alt="公式"> 层的所含单元个数</li>
<li><img src="https://www.zhihu.com/equation?tex=n%5E%7B%5B0%5D%7D=n_x" alt="公式">，表示输入层特征数目</li>
</ul>
<p>对应的<strong>反向传播</strong>过程中的 <img src="https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D" alt="公式"> 的维度分别是：</p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D:%5C%20(n%5E%7B%5Bl%5D%7D,n%5E%7B%5Bl-1%5D%7D)" alt="公式"></li>
<li><img src="https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D:%5C%20(n%5E%7B%5Bl%5D%7D,1)" alt="公式"></li>
<li>注意到， <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="公式"> 与 <img src="https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D" alt="公式"> 维度相同， <img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D" alt="公式"> 与 <img src="https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D" alt="公式"> 维度相同。这很容易理解。</li>
</ul>
<p><strong>正向传播过程中的 <img src="https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D" alt="公式"> 的维度分别是</strong>：</p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D:%5C%20(n%5E%7B%5Bl%5D%7D,1)" alt="公式"></li>
<li><img src="https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D:%5C%20(n%5E%7B%5Bl%5D%7D,1)" alt="公式"></li>
<li><img src="https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D" alt="公式"> 的维度是一样的，且 <img src="https://www.zhihu.com/equation?tex=dz%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=da%5E%7B%5Bl%5D%7D" alt="公式"> 的维度均与 <img src="https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D" alt="公式"> 的维度一致。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/img_convert/13daf26025fdc3819468a817f650becf.png" alt="深层神经网络前向运算"></p>
<p><strong>对于 <img src="https://www.zhihu.com/equation?tex=m" alt="公式"> 个训练样本</strong>，输入矩阵 <img src="https://www.zhihu.com/equation?tex=X" alt="公式"> 的维度是 <img src="https://www.zhihu.com/equation?tex=(n%5E%7B%5B0%5D%7D,m)" alt="公式">。需要注意的是 <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D" alt="公式"> 的维度与只有单个样本是一致的：</p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D:%5C%20(n%5E%7B%5Bl%5D%7D,n%5E%7B%5Bl-1%5D%7D)" alt="公式"></li>
<li><img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D:%5C%20(n%5E%7B%5Bl%5D%7D,1)" alt="公式"></li>
</ul>
<p>只不过在运算 <img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D=W%5E%7B%5Bl%5D%7DA%5E%7B%5Bl-1%5D%7D+b%5E%7B%5Bl%5D%7D" alt="公式"> 中， <img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D" alt="公式"> 会被当成 <img src="https://www.zhihu.com/equation?tex=(n%5E%7B%5Bl%5D%7D,m)" alt="公式"> 矩阵进行运算，这是基于python numpy的广播特性，且 <img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D" alt="公式"> 每一列向量都是一样的。 <img src="https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D" alt="公式"> 的维度分别与 <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D" alt="公式"> 的相同。</p>
<p>不过， <img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D" alt="公式"> 的维度发生了变化：</p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D:%5C%20(n%5E%7B%5Bl%5D%7D,m)" alt="公式"></li>
<li><img src="https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D:%5C%20(n%5E%7B%5Bl%5D%7D,m)" alt="公式"></li>
<li><img src="https://www.zhihu.com/equation?tex=dZ%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=dA%5E%7B%5Bl%5D%7D" alt="公式"> 的维度分别与 <img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D" alt="公式"> 的相同。</li>
</ul>
<h2 id="4-为什么需要深度网络"><a href="#4-为什么需要深度网络" class="headerlink" title="4.为什么需要深度网络"></a>4.为什么需要深度网络</h2><p>当今大家看到的很多AI智能场景背后都是巨大的神经网络在支撑，强大能力很大一部分来源于神经网络足够“深”，也就是说随着网络层数增多，神经网络就更加复杂参数更多，学习能力也更强。下面是一些典型的场景例子说明。</p>
<h3 id="4-1-人脸识别例子"><a href="#4-1-人脸识别例子" class="headerlink" title="4.1 人脸识别例子"></a>4.1 人脸识别例子</h3><p>如下图所示的<strong>人脸识别场景</strong>，训练得到的神经网络，每一层的作用有差别：</p>
<ul>
<li>第一层所做的事就是从原始图片中提取出<strong>人脸的轮廓与边缘</strong>，即<strong>边缘检测</strong>。这样每个神经元得到的是一些边缘信息。</li>
<li>第二层所做的事情就是<strong>将前一层的边缘进行组合</strong>，组合成<strong>人脸一些局部特征</strong>，比如眼睛、鼻子、嘴巴等。</li>
<li>后续层次逐层把这些<strong>局部特征组合起来</strong>，融合成人脸的模样。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/img_convert/73286a1c9b6585bdbb9eb5c8a6b588ff.png" alt="为什么需要深度网络"></p>
<p>可以看出，随着层数由浅到深，神经网络提取的特征也是<strong>从边缘到局部特征到整体，由简单到复杂</strong>。隐藏层越多，能够提取的特征就越丰富、越复杂，模型的准确率也可能会随之越高。</p>
<h3 id="4-2-语音识别例子"><a href="#4-2-语音识别例子" class="headerlink" title="4.2 语音识别例子"></a>4.2 语音识别例子</h3><p><strong>语音识别模型</strong>也是类似的道理：</p>
<ul>
<li>浅层的神经元能够检测一些简单的音调</li>
<li>较深的神经元能够检测出基本的音素</li>
<li>更深的神经元就能够检测出单词信息</li>
<li>网络足够深的话，还能对短语、句子进行检测</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/img_convert/57bc2518c4ab9bf06c59786b2bdf6ef3.png" alt="为什么需要深度网络"></p>
<p>神经网络从浅到深，提取的特征从简单到复杂。特征复杂度与神经网络层数成正相关。特征越来越复杂，表达能力和功能也越强。</p>
<h3 id="4-3-深度网络其他优势"><a href="#4-3-深度网络其他优势" class="headerlink" title="4.3 深度网络其他优势"></a>4.3 深度网络其他优势</h3><p>除学习能力与特征提取强度之外，深层网络还有另外一个优点，就是能够减少神经元个数，从而减少计算量。</p>
<p>下面有一个例子，使用电路理论，计算逻辑输出：</p>
<p><img src="https://www.zhihu.com/equation?tex=y=x_1%5Coplus%20x_2%5Coplus%20x_3%5Coplus%5Ccdots%5Coplus%20x_n" alt="公式"></p>
<ul>
<li>上面的计算表达式中， <img src="https://www.zhihu.com/equation?tex=%5Coplus" alt="公式"> 表示「<strong>异或</strong>」操作。</li>
</ul>
<p>对于这个逻辑运算，如果使用深度网络完成，每层将前一层的两两单元进行异或，最后到一个输出，如下图左边所示。</p>
<p>这样，整个深度网络的层数是 <img src="https://www.zhihu.com/equation?tex=log_2(n)" alt="公式"> (不包含输入层)。总共使用的神经元个数为：</p>
<p><img src="https://www.zhihu.com/equation?tex=1+2+%5Ccdots+2%5E%7Blog_2(n)-1%7D=1%5Ccdot%5Cfrac%7B1-2%5E%7Blog_2(n)%7D%7D%7B1-2%7D=2%5E%7Blog_2(n)%7D-1=n-1" alt="公式"></p>
<p>可见，输入个数是 <img src="https://www.zhihu.com/equation?tex=n" alt="公式">，这种深层网络所需的神经元个数仅仅是 <img src="https://www.zhihu.com/equation?tex=n-1" alt="公式"> 个。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/0bfafb21ce4984b41e53c219c863f783.png" alt="为什么需要深度网络"></p>
<p>如果不用深层网络，仅仅使用单个隐藏层，如上右图所示，由于包含了所有的逻辑位(0和1)，那么需要的神经元个数 <img src="https://www.zhihu.com/equation?tex=O(2%5En)" alt="公式"> 是指数级别的大小。</p>
<p>对于其他场景和问题也一样，处理同样的逻辑问题，深层网络所需的神经元个数比浅层网络要少很多。这也是深层神经网络的优点之一。</p>
<p>尽管深度学习有着非常显著的优势，吴恩达老师还是建议对实际问题进行建模时，尽量先选择层数少的神经网络模型，这也符合奥卡姆剃刀定律 (Occam’s Razor)。对于比较复杂的问题，再使用较深的神经网络模型。</p>
<h2 id="5-构建深度网络单元块"><a href="#5-构建深度网络单元块" class="headerlink" title="5.构建深度网络单元块"></a>5.构建深度网络单元块</h2><p>下面用流程块图来解释神经网络前向传播和反向传播过程。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/68352e6774da5595a53fbc6dd1cc50bb.png" alt="构建深度网络单元块"></p>
<p>如图所示，对于第 <img src="https://www.zhihu.com/equation?tex=l" alt="公式"> 层来说，<strong>前向传播</strong>过程中，我们有：</p>
<ul>
<li><strong>输入</strong>：<img src="https://www.zhihu.com/equation?tex=a%5E%7B%5Bl-1%5D%7D" alt="公式"></li>
<li><strong>输出</strong>：<img src="https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D" alt="公式"></li>
<li><strong>参数</strong>：<img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="公式"> 、 <img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D" alt="公式"></li>
<li><strong>缓存变量</strong>：<img src="https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D" alt="公式"></li>
</ul>
<p><strong>反向传播</strong>过程中：</p>
<ul>
<li><strong>输入</strong>：<img src="https://www.zhihu.com/equation?tex=da%5E%7B%5Bl%5D%7D" alt="公式"></li>
<li><strong>输出</strong>：<img src="https://www.zhihu.com/equation?tex=da%5E%7B%5Bl-1%5D%7D" alt="公式"> 、 <img src="https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D" alt="公式"> 、 <img src="https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D" alt="公式"></li>
<li><strong>参数</strong>：<img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="公式"> 、 <img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D" alt="公式"></li>
</ul>
<p>上面是第 <img src="https://www.zhihu.com/equation?tex=l" alt="公式"> 层的流程块图，对于神经网络所有层，整体的流程块图前向传播过程和反向传播过程如下所示：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/400c9534e57d223a6bde471b398db644.png" alt="构建深度网络单元块"></p>
<h2 id="6-前向传播与反向传播"><a href="#6-前向传播与反向传播" class="headerlink" title="6.前向传播与反向传播"></a>6.前向传播与反向传播</h2><p>我们继续接着上一部分流程块图的内容，推导神经网络正向传播过程和反向传播过程的具体表达式。</p>
<h3 id="6-1-前向传播过程"><a href="#6-1-前向传播过程" class="headerlink" title="6.1 前向传播过程"></a>6.1 前向传播过程</h3><p>令层数为第 <img src="https://www.zhihu.com/equation?tex=l" alt="公式"> 层，输入是 <img src="https://www.zhihu.com/equation?tex=a%5E%7B%5Bl-1%5D%7D" alt="公式">，输出是 <img src="https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D" alt="公式">，缓存变量是 <img src="https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D" alt="公式">。其表达式如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D=W%5E%7B%5Bl%5D%7Da%5E%7B%5Bl-1%5D%7D+b%5E%7B%5Bl%5D%7D" alt="公式"></p>
<p><img src="https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D=g%5E%7B%5Bl%5D%7D(z%5E%7B%5Bl%5D%7D)" alt="公式"></p>
<p><img src="https://www.zhihu.com/equation?tex=m" alt="公式"> <strong>个训练样本</strong>的形态下，<strong>向量化</strong>形式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D=W%5E%7B%5Bl%5D%7DA%5E%7B%5Bl-1%5D%7D+b%5E%7B%5Bl%5D%7D" alt="公式"></p>
<p><img src="https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D=g%5E%7B%5Bl%5D%7D(Z%5E%7B%5Bl%5D%7D)" alt="公式"></p>
<h3 id="6-2-反向传播过程"><a href="#6-2-反向传播过程" class="headerlink" title="6.2 反向传播过程"></a>6.2 反向传播过程</h3><p>输入是 <img src="https://www.zhihu.com/equation?tex=da%5E%7B%5Bl%5D%7D" alt="公式">，输出是 <img src="https://www.zhihu.com/equation?tex=da%5E%7B%5Bl-1%5D%7D" alt="公式"> 、 <img src="https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D" alt="公式"> 、 <img src="https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D" alt="公式">。其表达式如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=dz%5E%7B%5Bl%5D%7D=da%5E%7B%5Bl%5D%7D%5Cast%20g%5E%7B%5Bl%5D%5Cprime%7D(z%5E%7B%5Bl%5D%7D)" alt="公式"></p>
<p><img src="https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D=dz%5E%7B%5Bl%5D%7D%5Ccdot%20a%5E%7B%5Bl-1%5D%7D" alt="公式"></p>
<p><img src="https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D=dz%5E%7B%5Bl%5D%7D" alt="公式"></p>
<p><img src="https://www.zhihu.com/equation?tex=da%5E%7B%5Bl-1%5D%7D=W%5E%7B%5Bl%5DT%7D%5Ccdot%20dz%5E%7B%5Bl%5D%7D" alt="公式"></p>
<p>由上述第四个表达式可得 <img src="https://www.zhihu.com/equation?tex=da%5E%7B%5Bl%5D%7D=W%5E%7B%5Bl+1%5DT%7D%5Ccdot%20dz%5E%7B%5Bl+1%5D%7D" alt="公式">，将 <img src="https://www.zhihu.com/equation?tex=da%5E%7B%5Bl%5D%7D" alt="公式"> 代入第一个表达式中可以得到：</p>
<p><img src="https://www.zhihu.com/equation?tex=dz%5E%7B%5Bl%5D%7D=W%5E%7B%5Bl+1%5DT%7D%5Ccdot%20dz%5E%7B%5Bl+1%5D%7D%5Cast%20g%5E%7B%5Bl%5D%5Cprime%7D(z%5E%7B%5Bl%5D%7D)" alt="公式"></p>
<p>该式非常重要，反映了 <img src="https://www.zhihu.com/equation?tex=dz%5E%7B%5Bl+1%5D%7D" alt="公式"> 与 <img src="https://www.zhihu.com/equation?tex=dz%5E%7B%5Bl%5D%7D" alt="公式"> 的递推关系。</p>
<p><img src="https://www.zhihu.com/equation?tex=m" alt="公式"> <strong>个训练样本</strong>的形态下，<strong>向量化</strong>形式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=d%20Z%5E%7B%5Bl%5D%7D=d%20A%5E%7B%5Bl%5D%7D%5Cast%20g%5E%7B%5Bl%5D%5Cprime%7D%20(Z%5E%7B%5Bl%5D%7D)" alt="公式"></p>
<p><img src="https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D=%5Cfrac1mdZ%5E%7B%5Bl%5D%7D%5Ccdot%20A%5E%7B%5Bl-1%5DT%7D" alt="公式"></p>
<p><img src="https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D=%5Cfrac1mnp.sum(dZ%5E%7B%5Bl%5D%7D,axis=1,keepdim=True)" alt="公式"></p>
<p><img src="https://www.zhihu.com/equation?tex=dA%5E%7B%5Bl-1%5D%7D=W%5E%7B%5Bl%5DT%7D%5Ccdot%20dZ%5E%7B%5Bl%5D%7D" alt="公式"></p>
<p><img src="https://www.zhihu.com/equation?tex=dZ%5E%7B%5Bl%5D%7D=W%5E%7B%5Bl+1%5DT%7D%5Ccdot%20dZ%5E%7B%5Bl+1%5D%7D%5Cast%20g%5E%7B%5Bl%5D%5Cprime%7D(Z%5E%7B%5Bl%5D%7D)" alt="公式"></p>
<h2 id="7-参数与超参数"><a href="#7-参数与超参数" class="headerlink" title="7.参数与超参数"></a>7.参数与超参数</h2><p>神经网络中有两个大家要重点区分的概念：<strong>参数</strong>(<strong>parameters</strong>)和<strong>超参数</strong>(<strong>hyperparameters</strong>)。</p>
<ul>
<li>神经网络中的参数就是我们熟悉的 <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D" alt="公式">。</li>
<li>神经网络的超参数是例如学习率 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="公式">，训练迭代次数 <img src="https://www.zhihu.com/equation?tex=N" alt="公式">，神经网络层数 <img src="https://www.zhihu.com/equation?tex=L" alt="公式">，各层神经元个数 <img src="https://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D" alt="公式">，激活函数 <img src="https://www.zhihu.com/equation?tex=g(z)" alt="公式"> 等。</li>
<li>之所以叫做超参数，是因为它们需要提前敲定，而且它们会决定参数 <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="公式"> 和 <img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D" alt="公式"> 的值。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/img_convert/4ac1059666e255e2bead507164482c29.png" alt="参数与超参数"></p>
<p>如何设置最优的超参数是一个比较困难的、需要经验知识的问题。通常的做法是选择超参数一定范围内的值，分别代入神经网络进行训练，测试cost function随着迭代次数增加的变化，根据结果选择cost function最小时对应的超参数值。这类似于机器学习中的实验验证的方法。</p>
<h2 id="8-神经网络vs人脑"><a href="#8-神经网络vs人脑" class="headerlink" title="8.神经网络vs人脑"></a>8.神经网络vs人脑</h2><p>神经网络跟人脑机制到底有什么联系呢？究竟有多少的相似程度？</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/e630b075438c9ae623890c7ea95d1edd.png" alt="深度网络 VS 大脑"></p>
<p>我们前面看到神经网络实际上可以分成两个部分：<strong>前向传播</strong>过程和<strong>反向传播</strong>过程。神经网络的每个神经元采用激活函数的方式，类似于感知机模型。这种模型与人脑神经元是类似的，但是一种非常简化的人脑神经元模型。</p>
<p><strong>人脑神经元可分为树突、细胞体、轴突三部分</strong>。树突接收外界电刺激信号(类比神经网络中神经元输入)，传递给细胞体进行处理(类比神经网络中神经元激活函数运算)，最后由轴突传递给下一个神经元(类比神经网络中神经元输出)。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/83fb4374f57e3227de58cd11493a4df3.png" alt="深度网络 VS 大脑"></p>
<p><strong>人脑神经元的结构和处理方式要复杂的多，神经网络模型只是非常简化的模型</strong>。</p>
<p>人脑如何进行学习？是否也是通过反向传播和梯度下降算法现在还不清楚，可能会更加复杂。这是值得生物学家探索的事情。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://olstussy.top">Olstussy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://olstussy.top/2023/08/03/4.%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">https://olstussy.top/2023/08/03/4.深层神经网络/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://olstussy.top" target="_blank">Olstussy</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2023/08/02/UOWRIZPuaAX81yL.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/08/03/5.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8/" title="AI笔记（5）：神经网络应用"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">AI笔记（5）：神经网络应用</div></div></a></div><div class="next-post pull-right"><a href="/2023/08/03/3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="AI笔记（3）：浅层神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">AI笔记（3）：浅层神经网络</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/08/03/3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="AI笔记（3）：浅层神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-03</div><div class="title">AI笔记（3）：浅层神经网络</div></div></a></div><div><a href="/2023/08/02/2.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" title="AI笔记（2）：神经网络基础"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-02</div><div class="title">AI笔记（2）：神经网络基础</div></div></a></div><div><a href="/2023/08/03/5.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8/" title="AI笔记（5）：神经网络应用"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-03</div><div class="title">AI笔记（5）：神经网络应用</div></div></a></div><div><a href="/2023/08/01/1.%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86,%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/" title="AI笔记（1）：基础知识"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-01</div><div class="title">AI笔记（1）：基础知识</div></div></a></div><div><a href="/2023/08/06/cs231n(%E4%B8%80)_part1/" title="cs231n(一)_part1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-06</div><div class="title">cs231n(一)_part1</div></div></a></div><div><a href="/2023/08/06/cs231n(%E4%B8%80)_part2/" title="cs231n(一)_part2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-06</div><div class="title">cs231n(一)_part2</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2023/08/02/UOWRIZPuaAX81yL.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Olstussy</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/olstussy"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">No Error No Warning !</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.</span> <span class="toc-text">1.深层神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%89%8D%E5%90%91%E8%BF%90%E7%AE%97"><span class="toc-number">2.</span> <span class="toc-text">2.深层神经网络前向运算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%8D%95%E4%B8%AA%E6%A0%B7%E6%9C%AC%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 单个样本的计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-m%E4%B8%AA%E6%A0%B7%E6%9C%AC%E7%9A%84%E6%89%B9%E9%87%8F%E8%AE%A1%E7%AE%97"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 m个样本的批量计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%90%91%E9%87%8F%E5%8C%96%E5%BD%A2%E6%80%81%E4%B8%8B%E7%9A%84%E7%9F%A9%E9%98%B5%E7%BB%B4%E5%BA%A6"><span class="toc-number">3.</span> <span class="toc-text">3.向量化形态下的矩阵维度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C"><span class="toc-number">4.</span> <span class="toc-text">4.为什么需要深度网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E4%BE%8B%E5%AD%90"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 人脸识别例子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E4%BE%8B%E5%AD%90"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 语音识别例子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E5%85%B6%E4%BB%96%E4%BC%98%E5%8A%BF"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 深度网络其他优势</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E5%8D%95%E5%85%83%E5%9D%97"><span class="toc-number">5.</span> <span class="toc-text">5.构建深度网络单元块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">6.</span> <span class="toc-text">6.前向传播与反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B"><span class="toc-number">6.1.</span> <span class="toc-text">6.1 前向传播过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B"><span class="toc-number">6.2.</span> <span class="toc-text">6.2 反向传播过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E5%8F%82%E6%95%B0%E4%B8%8E%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-number">7.</span> <span class="toc-text">7.参数与超参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cvs%E4%BA%BA%E8%84%91"><span class="toc-number">8.</span> <span class="toc-text">8.神经网络vs人脑</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/09/10/%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/" title="AI经典论文综述">AI经典论文综述</a><time datetime="2023-09-10T04:40:21.000Z" title="发表于 2023-09-10 12:40:21">2023-09-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/09/10/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1-7%E7%AB%A0)/" title="统计学习笔记(1-7章)">统计学习笔记(1-7章)</a><time datetime="2023-09-10T02:36:21.000Z" title="发表于 2023-09-10 10:36:21">2023-09-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/08/20/cs231n(%E4%B8%89)/" title="cs231n(三)">cs231n(三)</a><time datetime="2023-08-20T02:10:21.000Z" title="发表于 2023-08-20 10:10:21">2023-08-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/08/20/cs231n(%E4%BA%8C)/" title="cs231n(二)">cs231n(二)</a><time datetime="2023-08-20T02:07:21.000Z" title="发表于 2023-08-20 10:07:21">2023-08-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/08/06/cs231n(%E4%B8%80)_part2/" title="cs231n(一)_part2">cs231n(一)_part2</a><time datetime="2023-08-06T05:40:21.000Z" title="发表于 2023-08-06 13:40:21">2023-08-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By Olstussy</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>